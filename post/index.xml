<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Jianchao Li</title>
    <link>https://jianchao-li.github.io/post/</link>
    <description>Recent content in Posts on Jianchao Li</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year}</copyright>
    <lastBuildDate>Sat, 29 Feb 2020 10:49:18 +0800</lastBuildDate>
    
	    <atom:link href="https://jianchao-li.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Interpret PyTorch Models with Captum</title>
      <link>https://jianchao-li.github.io/post/interpret-pytorch-models-with-captum/</link>
      <pubDate>Sat, 29 Feb 2020 10:49:18 +0800</pubDate>
      
      <guid>https://jianchao-li.github.io/post/interpret-pytorch-models-with-captum/</guid>
      <description>

&lt;p&gt;While deep neural networks have achieved state-of-the-art performance in many problems(e.g., image classification, object detection, scene parsing etc.), it is always not trivial to intepret their outputs. Till now, the most common and useful way to interpret the output of a deep neural network is still by visualization. You may refer to this &lt;a href=&#34;http://cs231n.github.io/understanding-cnn/&#34; target=&#34;_blank&#34;&gt;CS231n course note&lt;/a&gt; for some introduction.&lt;/p&gt;

&lt;p&gt;In this post, I will describe how to interpret an image classification model using &lt;a href=&#34;https://captum.ai/&#34; target=&#34;_blank&#34;&gt;Captum&lt;/a&gt;. Captum, which means &amp;ldquo;comprehension&amp;rdquo; in Latin, is a open-source project with many model interpretabiliy algorithms implemented in PyTorch. Specifically, I adopted &lt;a href=&#34;https://github.com/pytorch/captum/blob/master/captum/attr/_core/layer/grad_cam.py#L21&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;LayerGradCam&lt;/code&gt;&lt;/a&gt; for this post.&lt;/p&gt;

&lt;h2 id=&#34;install-captum&#34;&gt;Install Captum&lt;/h2&gt;

&lt;p&gt;As &lt;code&gt;LayerGradCam&lt;/code&gt; is still not released at the time of writing this post, to use it, clone the Captum repository locally and install it from there.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;git clone git@github.com:pytorch/captum.git
cd captum
pip install -e .&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then import all the required packages.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; requests

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; io &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; BytesIO

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; np

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torchvision &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; models, transforms

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;matplotlib inline

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; captum.attr &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LayerAttribution, LayerGradCam&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;prepare-a-model-and-an-image&#34;&gt;Prepare a Model and an Image&lt;/h2&gt;

&lt;p&gt;I use the &lt;a href=&#34;https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenet.py#L72&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;MobileNetV2&lt;/code&gt;&lt;/a&gt; pretrained on ImageNet from &lt;code&gt;torchvision&lt;/code&gt; and an image of a Hornbill from &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/8/8f/Buceros_bicornis_%28female%29_-feeding_in_tree-8.jpg&#34; target=&#34;_blank&#34;&gt;Wikipedia&lt;/a&gt;. Later I will use &lt;code&gt;LayerGradCam&lt;/code&gt; to intepret and visualize why the model gives the specific output for this image.&lt;/p&gt;

&lt;p&gt;Note that the model needs to be set to the test mode.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# use MobileNetV2&lt;/span&gt;
model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; models&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mobilenet_v2(pretrained&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;eval()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For the image, I first read its encoded string from its URL and then use the &lt;code&gt;PIL.Image&lt;/code&gt; format to decode it. In this way, the channels of the image are in the RGB order.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;img_url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://upload.wikimedia.org/wikipedia/commons/8/8f/Buceros_bicornis_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%28f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;emale%29_-feeding_in_tree-8.jpg&amp;#39;&lt;/span&gt;
resp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(img_url)
img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(BytesIO(resp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;content))
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;figure(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;imshow(img)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;image.png&#34;/&gt;&lt;/div&gt;

&lt;p&gt;I also prepare the class names for the 1000 classes in ImageNet. This will let me know the specific class names instead of only the index of the predicted class. The class names are loaded from the following URL.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&amp;#39;&lt;/span&gt;
resp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(url)
class_names_map &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;loads(resp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;text)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;

&lt;p&gt;For &lt;code&gt;torchvision&lt;/code&gt; models, before passing an image to it, the image needs to be applied the following preprocessing (&lt;a href=&#34;https://github.com/pytorch/vision/issues/39#issuecomment-403701432&#34; target=&#34;_blank&#34;&gt;reference&lt;/a&gt;). This is a key step to make the model run on images from the same distribution as of those that it was trained on.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;preprocessing &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; transforms&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Compose([
    transforms&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;),
    transforms&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CenterCrop(&lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;),
    transforms&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ToTensor(),
    transforms&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Normalize(
        mean&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;0.485&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.456&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.406&lt;/span&gt;],
        std&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;0.229&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.224&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.225&lt;/span&gt;]
    ),
])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;layergradcam&#34;&gt;LayerGradCam&lt;/h2&gt;

&lt;p&gt;Now we can apply &lt;code&gt;LayerGradCam&lt;/code&gt; to &amp;ldquo;attribute&amp;rdquo; the output of the model to a specific layer of the model. What &lt;code&gt;LayerGradCam&lt;/code&gt; does is basically computing the gradients of the output with respect to that specific layer. The following function is used to get a layer from the model by its name.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_layer&lt;/span&gt;(model, layer_name):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; name &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; layer_name&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.&amp;#34;&lt;/span&gt;):
        model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; getattr(model, name)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; model&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;features.18&lt;/code&gt; layer of MobileNetV2 will be used in this notebook.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;layer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_layer(model, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;features.18&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We will use &lt;code&gt;LayerGradCam&lt;/code&gt; to compute the attribution map (gradients) of the model&amp;rsquo;s top-1 output with respect to &lt;code&gt;layer&lt;/code&gt;. This map can be interpreted as to what extent is the output influenced by a unit in &lt;code&gt;layer&lt;/code&gt;. This makes sense as the larger the gradient, the larger the influence.&lt;/p&gt;

&lt;p&gt;This attribution map (with the same size as the output of &lt;code&gt;layer&lt;/code&gt;, in this case, 7*7) is further upsampled to the size of the image and overlaid on the image as a heatmap. So this heatmap reflects how much influence each pixel has on the output of the model. The pixels with larger influence (the red regions in the heatmap) can thus be interpreted as the main regions in the image that drive the model to generate its output.&lt;/p&gt;

&lt;p&gt;To enable all above processing of the attribution map, two functions are implemented as follows. The first function &lt;code&gt;to_gray_image&lt;/code&gt; converts an &lt;code&gt;np.array&lt;/code&gt; to a gray-scale image by normalizing its values to &lt;code&gt;[0, 1]&lt;/code&gt;, multiplying it by 255, and converting its data type to &lt;code&gt;uint8&lt;/code&gt;. The second one &lt;code&gt;compute_heatmap&lt;/code&gt; utilizes &lt;code&gt;cv2&lt;/code&gt; to overlay a &lt;code&gt;torch.Tensor&lt;/code&gt; as a heatmap on an image.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;to_gray_image&lt;/span&gt;(x):
    x &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;min()
    x &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max() &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;spacing(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    x &lt;span style=&#34;color:#f92672&#34;&gt;*=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(x, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uint8)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;overlay_heatmap&lt;/span&gt;(img, grad):
    &lt;span style=&#34;color:#75715e&#34;&gt;# convert PIL Image to numpy array&lt;/span&gt;
    img_np &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(img)
    &lt;span style=&#34;color:#75715e&#34;&gt;# convert gradients to heatmap&lt;/span&gt;
    grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;squeeze()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;detach()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;numpy()
    grad_img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; to_gray_image(grad)
    heatmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;applyColorMap(grad_img, cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COLORMAP_JET)
    heatmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; heatmap[:, :, ::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#75715e&#34;&gt;# convert to rgb&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# overlay heatmap on image&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;addWeighted(img_np, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, heatmap, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In &lt;code&gt;overlay_heatmap&lt;/code&gt;, note that &lt;code&gt;img&lt;/code&gt; is in RGB order while the &lt;code&gt;heatmap&lt;/code&gt; returned by &lt;code&gt;cv2.applyColorMap&lt;/code&gt; is in BGR order. So we convert &lt;code&gt;heatmap&lt;/code&gt; to RGB order first before the overlay.&lt;/p&gt;

&lt;p&gt;Using all above functions, the following function &lt;code&gt;attribute&lt;/code&gt; computes and overlays the &lt;code&gt;LayerGradCam&lt;/code&gt; heatmap on an image.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;attribute&lt;/span&gt;(img):
    &lt;span style=&#34;color:#75715e&#34;&gt;# preprocess the image&lt;/span&gt;
    preproc_img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; preprocessing(img)
    &lt;span style=&#34;color:#75715e&#34;&gt;# forward propagation to get the model outputs&lt;/span&gt;
    inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; preproc_img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
    out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model(inp)
    &lt;span style=&#34;color:#75715e&#34;&gt;# construct LayerGradCam&lt;/span&gt;
    layer_grad_cam &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LayerGradCam(model, layer)
    &lt;span style=&#34;color:#75715e&#34;&gt;# generate attribution map&lt;/span&gt;
    _, out_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;topk(out, k&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    out_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; out_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;squeeze(dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
    attr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; layer_grad_cam&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;attribute(inp, out_index)
    upsampled_attr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LayerAttribution&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;interpolate(attr, (img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;height, img&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;width), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bicubic&amp;#39;&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;# generate heatmap&lt;/span&gt;
    heatmap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; overlay_heatmap(img, upsampled_attr)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; heatmap, out_index&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Specifically, what &lt;code&gt;attribute&lt;/code&gt; does is as follows.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Preprocess the image;&lt;/li&gt;
&lt;li&gt;Run a forward propagation on the image to get the model&amp;rsquo;s output;&lt;/li&gt;
&lt;li&gt;Construct a &lt;code&gt;LayerGradCam&lt;/code&gt; object using &lt;code&gt;model&lt;/code&gt; and &lt;code&gt;layer&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Generate the attribution map of the model&amp;rsquo;s top-1 output to &lt;code&gt;layer&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Upsample the attribution map to the same size as the image;&lt;/li&gt;
&lt;li&gt;Overlay the attribution map as a heatmap on the image.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now it is time to run an example! Let&amp;rsquo;s see what class the &lt;code&gt;model&lt;/code&gt; predicts on the Hornbill image, and more importantly, why.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vis, out_index = attribute(img)
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
ax.set_title(class_names_map[str(out_index)], fontsize=30)
plt.imshow(vis)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;visualization.png&#34;/&gt;&lt;/div&gt;

&lt;p&gt;We can see that the model makes a correct prediction. From the above visualization, we can also see that the red regions are mostly around the head and beak of the Hornbill, especiall its heavy bill. The red regions are the main regions that drive the model to generate its output. This makes great sense as those regions are just the distinctive features of a Hornbill.&lt;/p&gt;

&lt;p&gt;Now you can also apply the above technique (and more from Captum) to interpret the output of your PyTorch model. Have fun!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt; This post is alao avaialble as a &lt;a href=&#34;https://gist.github.com/jianchao-li/f7b507bc66b2215e15cc0135f03c3ff9&#34; target=&#34;_blank&#34;&gt;Jupyter notebook&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From PyTorch to Core ML</title>
      <link>https://jianchao-li.github.io/post/from-pytorch-to-coreml/</link>
      <pubDate>Wed, 16 Oct 2019 01:00:18 +0800</pubDate>
      
      <guid>https://jianchao-li.github.io/post/from-pytorch-to-coreml/</guid>
      <description>

&lt;p&gt;After working with PyTorch in my daily work for some time, recently I got a chance to work on something completely new - &lt;a href=&#34;https://developer.apple.com/documentation/coreml&#34; target=&#34;_blank&#34;&gt;Core ML&lt;/a&gt;. After converting a PyTorch model to the Core ML format and seeing it work in an iPhone 7, I believe this deserves a blog post.&lt;/p&gt;

&lt;h2 id=&#34;what-is-core-ml&#34;&gt;What is Core ML?&lt;/h2&gt;

&lt;p&gt;Core ML is a framework developed by Apple to integrate machine learning models into iOS applications. As like each other framework, Core ML has its own model format (&lt;code&gt;.mlmodel&lt;/code&gt;), like &lt;code&gt;.pth&lt;/code&gt; of PyTorch or &lt;code&gt;.params&lt;/code&gt; of MXNet.&lt;/p&gt;

&lt;p&gt;Compared to PyTorch or MXNet, Core ML is mainly used as an inference engine in iOS. That means you will first train a model using PyTorch (&lt;code&gt;.pth&lt;/code&gt;) or MXNet (&lt;code&gt;.params&lt;/code&gt;) and then convert it to the Core ML format (&lt;code&gt;.mlmodel&lt;/code&gt;) and deploy it to an iOS app.&lt;/p&gt;

&lt;h2 id=&#34;get-a-sense-first&#34;&gt;Get a Sense First&lt;/h2&gt;

&lt;p&gt;Before diving into details, it is better to get a sense of what a Core ML model looks like. You may download the sample code in &lt;a href=&#34;https://developer.apple.com/documentation/vision/classifying_images_with_vision_and_core_ml&#34; target=&#34;_blank&#34;&gt;Classifying Images with Vision and Core ML&lt;/a&gt;. There is a &lt;code&gt;MobileNet.mlmodel&lt;/code&gt; inside it. You can open it with Xcode to see what it looks like.&lt;/p&gt;

&lt;p&gt;The following is a screenshot of the model details. In the center area, there are 3 sections: Machine Learning Model, Model Class and Prediction.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;model_details.png&#34;/&gt;&lt;/div&gt;

&lt;p&gt;The interesting part is the Prediction. It tells us that the input to the model is a color (RGB) image of size 224 x 224 and the outputs have two parts: top-1 category &lt;code&gt;classLabel&lt;/code&gt; and the probabilities of all categories &lt;code&gt;classLabelProbs&lt;/code&gt;. &lt;strong&gt;This will guide the model conversion later&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Then ou can click the triangle in the following red rectangle to build the project. You can also select the device simulator that you want to run the project on in the blue rectangle.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;xcode.png&#34;/&gt;&lt;/div&gt;

&lt;p&gt;You may need to configure the &amp;ldquo;Signing &amp;amp; Capabilities&amp;rdquo; by clicking the &lt;code&gt;Vision+ML Example&lt;/code&gt; folder (an Apple ID will be needed). After that, you should see an iPhone coming out in your screen and you can start to add a photo and play with it! If you want to try it on a real iPhone, just connect your iPhone to the computer (USB or Type-C) then you should be able to select your iPhone in the blue rectangle.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;iphone.png&#34; width=&#34;30%&#34;/&gt;&lt;/div&gt;

&lt;p&gt;You can try more open source Core ML models &lt;a href=&#34;https://developer.apple.com/machine-learning/models/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. To add a model to the project, you need to drag it to the project structure and set it up as follows. Some files will be generated automatically for you to use the model.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;drag.png&#34;/&gt;&lt;/div&gt;

&lt;p&gt;You need to change the line &lt;code&gt;let model = try VNCoreMLModel(for: MobileNet().model)&lt;/code&gt; in &lt;code&gt;ImageClassificationViewController.swift&lt;/code&gt; to use the model. You may also need to update the target iOS version shown in the red rectangle of the following screenshot.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;ios_version.png&#34;/&gt;&lt;/div&gt;

&lt;h2 id=&#34;model-conversion&#34;&gt;Model Conversion&lt;/h2&gt;

&lt;p&gt;Now we take a step back. We have just trained a model using PyTorch or MXNet and we wwant to run it on iOS. Obviously, we need to convert the &lt;code&gt;.pth&lt;/code&gt; or &lt;code&gt;.params&lt;/code&gt; to &lt;code&gt;.mlmodel&lt;/code&gt;. This is model conversion.&lt;/p&gt;

&lt;p&gt;For Caffe and Keras, their models can be converted to Core ML models directly. However, such direct conversion is not supported for PyTorch. Fortunately, we have &lt;a href=&#34;https://onnx.ai/&#34; target=&#34;_blank&#34;&gt;ONNX&lt;/a&gt;, an excellent exchange format between models of various frameworks.&lt;/p&gt;

&lt;p&gt;The conversion flow from PyTorch to Core ML is as follows. I will use the &lt;code&gt;mobilenet_v2&lt;/code&gt; of &lt;code&gt;torchvision&lt;/code&gt; as an example to walk through the conversion process.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;conversion.png&#34;/&gt;&lt;/div&gt;

&lt;h3 id=&#34;loading-torchvision-model&#34;&gt;Loading TorchVision Model&lt;/h3&gt;

&lt;p&gt;First I load a MobileNet v2 pretrained on ImageNet. Note that I add a Softmax layer to get the probabilities of all categories (remember by the output &lt;code&gt;classLabelProbs&lt;/code&gt; of the Core ML model?).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.nn &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; nn
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torchvision

model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torchvision&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;models&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mobilenet_v2(pretrained&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;span style=&#34;color:#75715e&#34;&gt;# torchvision models do not have softmax outputs&lt;/span&gt;
model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential(model, nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Softmax())&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;pytorch-to-onnx&#34;&gt;PyTorch to ONNX&lt;/h3&gt;

&lt;p&gt;Then I convert the above PyTorch model to onnx (&lt;code&gt;model.onnx&lt;/code&gt;). Note that the &lt;code&gt;input_names&lt;/code&gt; and &lt;code&gt;output_names&lt;/code&gt; are consistent with the above Core ML model.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;dummy_input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;)
torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;onnx&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;export(model, dummy_input, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mobilenet_v2.onnx&amp;#39;&lt;/span&gt;, verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True,
                  input_names&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;image&amp;#39;&lt;/span&gt;], output_names&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;classLabelProbs&amp;#39;&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;onnx-to-core-ml&#34;&gt;ONNX to Core ML&lt;/h3&gt;

&lt;p&gt;Finally, convert the ONNX model to a Core ML model (&lt;code&gt;mobilenet_v2.mlmodel&lt;/code&gt;). In this process, the class labels of ImageNet is required, which can be dowloaded to &lt;code&gt;imagenet_class_index.json&lt;/code&gt; from &lt;a href=&#34;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. The &lt;code&gt;image_input_names=[&#39;image&#39;]&lt;/code&gt; means we treat the &lt;code&gt;image&lt;/code&gt; (input of the onnx model) as an image (remember the input &lt;code&gt;image&lt;/code&gt; of the above Core ML model?). &lt;code&gt;predicted_feature_name=&#39;classLabel&#39;&lt;/code&gt; will generate the other output of the above Core ML model.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; requests
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; onnx_coreml &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; convert

IMAGENET_CLASS_INDEX_URL &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;load_imagenet_class_labels&lt;/span&gt;():
    response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(IMAGENET_CLASS_INDEX_URL)
    index_json &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;json()
    class_labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [index_json[str(i)][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;)]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; class_labels

model_onnx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; onnx&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mobilenet_v2.onnx&amp;#39;&lt;/span&gt;)
class_labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_imagenet_class_labels()
model_coreml &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; convert(model_onnx, mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;, image_input_names&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;image&amp;#39;&lt;/span&gt;],
	               class_labels&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;class_labels, predicted_feature_name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;classLabel&amp;#39;&lt;/span&gt;)
model_coreml&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mobilenet_v2.mlmodel&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you can drag the &lt;code&gt;mobilenet_v2.mlmodel&lt;/code&gt; to your project and play with it. Have fun!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Connecting Computer Vision and Natural Language</title>
      <link>https://jianchao-li.github.io/post/connecting-computer-vision-and-natural-language/</link>
      <pubDate>Tue, 27 Nov 2018 15:36:00 +0800</pubDate>
      
      <guid>https://jianchao-li.github.io/post/connecting-computer-vision-and-natural-language/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In recent years, computer vision has witnessed extremely rapid progress. Somewhat surprisingly, this thriving field was originated from &lt;a href=&#34;https://dspace.mit.edu/handle/1721.1/6125&#34; target=&#34;_blank&#34;&gt;a summer project at MIT in 1966&lt;/a&gt;. Richard Szeliski wrote in &lt;a href=&#34;http://szeliski.org/Book/&#34; target=&#34;_blank&#34;&gt;Computer Vision: Algorithms and Applications&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;in 1966, Marvin Minsky at MIT asked his undergraduate student Gerald Jay Sussman to spend the summer linking a camera to a computer and &lt;strong&gt;getting the computer to describe what it saw&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This &lt;em&gt;see-and-describe&lt;/em&gt; summarizes the original goal of the pioneers: let the computer see the world around it (expressed in images/videos) and describe it.&lt;/p&gt;

&lt;p&gt;Till now, several granularities of descriptions have been developed: image-level category descriptions (image classification), object-level location descriptions (object detection), and pixel-level dense descriptions (image segmentation).&lt;/p&gt;

&lt;p&gt;However, the most natural way to describe something (for humans) is to use &lt;em&gt;natural language&lt;/em&gt;. Actually, the above story of Marvin Minsky, though often been cited as an evidence of how those masters underestimated the difficulties of vision problems, also shows that computer vision was born with an expectation of being connected with natural language.&lt;/p&gt;

&lt;h2 id=&#34;current-research&#34;&gt;Current Research&lt;/h2&gt;

&lt;p&gt;Many researchers have been seeking to build the connection between computer vision and natural language, which poses a challenging modeling problem with two modalities of data (images and natural language). Nowadays, the research community has generally come to a consensus on modeling images with convolutional neural networks (CNNs) and natural language with recurrent neural networks (RNNs). Both of these architectures can be made deeper by adding layers with homogeneous computations for better performance. Specifically, researchers have tried to build the connection between vision and natural language in the following ways.&lt;/p&gt;

&lt;h3 id=&#34;image-video-captioning&#34;&gt;Image/Video Captioning&lt;/h3&gt;

&lt;p&gt;In image/video captioning, an image/video is given and a sentence describing its content is returned. In current research, the image/video is usually encoded into a feature vector by a CNN. Then an RNN generates the captions using this vector as the initial hidden state, as shown in the following two figures (taken from &lt;a href=&#34;https://cs.stanford.edu/people/karpathy/deepimagesent/&#34; target=&#34;_blank&#34;&gt;Deep Visual-Semantic Alignments for Generating Image Descriptions&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1412.4729&#34; target=&#34;_blank&#34;&gt;Translating Videos to Natural Language Using Deep Recurrent Neural Networks&lt;/a&gt;).&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;imagecap.png&#34;/&gt;&lt;/div&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;videocap.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;h3 id=&#34;image-generation-from-text&#34;&gt;Image Generation (from Text)&lt;/h3&gt;

&lt;p&gt;This is the inverse problem of image captioning: a sentence is given and an image matching the meaning of the sentence is returned. The recent advances of generative adversarial networks (GANs) have opened up tons of opportunities for generation tasks like this. Typically, image generation makes use of GANs with the text being encoded by an RNN and fed into the generator/discriminator networks, as shown below (taken from &lt;a href=&#34;https://arxiv.org/abs/1605.05396&#34; target=&#34;_blank&#34;&gt;Generative Adversarial Text to Image Synthesis&lt;/a&gt;).&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;text2im.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;h3 id=&#34;visual-question-answering&#34;&gt;Visual Question Answering&lt;/h3&gt;

&lt;p&gt;In visual question answering (VQA), an image and a question about it are given and the answer is returned. This is arguably a very natural way for humans to interact with computers. In recent years, computers have learned to answer questions like &lt;em&gt;is this a cat&lt;/em&gt; (classification) or &lt;em&gt;where is the cat&lt;/em&gt; (detection/segmentation). Now they are asked more questions like &lt;a href=&#34;http://www.visualqa.org/&#34; target=&#34;_blank&#34;&gt;counting&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/people/jcjohns/clevr/&#34; target=&#34;_blank&#34;&gt;spatial/logical reasoning&lt;/a&gt;, and &lt;a href=&#34;https://datasets.maluuba.com/FigureQA&#34; target=&#34;_blank&#34;&gt;analyzing graphical plots and figures&lt;/a&gt;. In VQA, the visual content and the question content are often encoded by CNNs and RNNs respectively and then combined in some way to generate the answer, as shown below (taken from &lt;a href=&#34;https://arxiv.org/abs/1707.07998&#34; target=&#34;_blank&#34;&gt;Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering&lt;/a&gt;).&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;butdvqa.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;h2 id=&#34;future-opportunities&#34;&gt;Future Opportunities&lt;/h2&gt;

&lt;p&gt;Current research in connecting computer vision and natural language has achieved great breakthroughs thanks to the success of CNNs and RNNs in the two areas respectively. However, there are still some limitations in current research that open up future opportunities.&lt;/p&gt;

&lt;h3 id=&#34;fine-grained-image-captioning&#34;&gt;Fine-grained Image Captioning&lt;/h3&gt;

&lt;p&gt;Current image captioning generates captions which give an overall description of images. The results of applying &lt;a href=&#34;https://github.com/karpathy/neuraltalk2&#34; target=&#34;_blank&#34;&gt;a state-of-the-art image captioning algorithm&lt;/a&gt; to images from the &lt;a href=&#34;http://cocodataset.org/&#34; target=&#34;_blank&#34;&gt;COCO dataset&lt;/a&gt; are shown below.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;cococaps.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;p&gt;In the COCO dataset, images are of various scenes and objects. And the captioning algorithm is able to capture the overall content of what is happening in the image, except for some mistakes like the cat is not sitting on the laptop. But, in general, the captions are very discriminative considering the large differences between images. Given images and captions, it is very easy to tell which image corresponds to which caption. Image captioning makes great sense in this case.&lt;/p&gt;

&lt;p&gt;Then I applied the same captioning algorithm to the &lt;a href=&#34;https://github.com/bearpaw/clothing-co-parsing&#34; target=&#34;_blank&#34;&gt;Clothing Co-Parsing (CCP) dataset&lt;/a&gt;, whose images are all clothing images. The captions are shown below.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;ccpcaps.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;p&gt;In the CCP dataset, images are all coming from the clothing domain and thus they are very similar to each other in the overall content. And the differences are mostly reflected in fine-grained details. In this case, the captions which only capture the overall content become meaningless and are not very helpful for distinguishing one image from others. Moreover, the captions make more mistakes, like a lot of false positives of cell phones.&lt;/p&gt;

&lt;p&gt;For classifying images in the same domain, researchers have come up with fine-grained image classification. Now to caption these images, whose fine-grained details are much more important than the overall content, it makes sense to state that we need &lt;em&gt;fine-grained image captioning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Similar to fine-grained image classification, which finds many applications in online advertising (like searching by images), fine-grained image captioning also finds an important application in this area, that is, to write captions for goods.&lt;/p&gt;

&lt;p&gt;Actually, businesses are always trying to describe the attractive details of their goods to convince customers to make the buying decision. For example, the advertising captions of two clothing images in &lt;a href=&#34;http://m.toutiao.com/profile/5569547953/&#34; target=&#34;_blank&#34;&gt;Toutiao&lt;/a&gt; are shown below.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;toutiaocaps.png&#34;/&gt;&lt;/div&gt;

&lt;p&gt;The above captions are very different from those of the COCO and CCP datasets. Instead of merely focusing on the overall image content, they try to capture more fine-grained details of the clothes. They even go beyond those details to present customers a sense of how the clothes will look on him/her. These captions are also more flexible since they are manually written by businesses, though a mistake about the color of the dress is made in the right image. So a natural question is whether we can apply image captioning to write such captions for advertising. Obviously, general image captioning is still unable to perform well on it, as shown in the captions of the CCP dataset. So fine-grained image captioning comes into use. However, there are still very few works on it.&lt;/p&gt;

&lt;p&gt;It is worth noting that though I am using clothing as an example domain to present fine-grained image captioning, it is definitely not limited to clothing and can also be applied to many other domains like &lt;a href=&#34;https://arxiv.org/abs/1512.02665&#34; target=&#34;_blank&#34;&gt;food&lt;/a&gt; and &lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html&#34; target=&#34;_blank&#34;&gt;cars&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To solve the fine-grained image captioning problem, the considerable number of online advertising captions serve as a good basis. The pipeline of fine-grained image captioning may also be similar to that of general image captioning: a CNN learns a domain-specific representation of the image (maybe via fine-tuning the network in a fine-grained image classification task) and then an RNN generates a fine-grained caption conditioned on the representation. There should be many problems waiting to be discovered and solved in fine-grained image captioning.&lt;/p&gt;

&lt;h3 id=&#34;short-videos&#34;&gt;Short Videos&lt;/h3&gt;

&lt;p&gt;Recent years, we have witnessed an increasing popularity of short videos. Many companies like Facebook, Instagram, &lt;a href=&#34;https://www.kuaishou.com/&#34; target=&#34;_blank&#34;&gt;Kuaishou&lt;/a&gt;, &lt;a href=&#34;https://www.douyin.com/&#34; target=&#34;_blank&#34;&gt;Douyin&lt;/a&gt; etc. have developed products to enable their users to upload and share short videos. Compared to static images and long videos, short videos have the flexibility and authenticity of videos, and can also be as concentrated (on a topic) as static images.&lt;/p&gt;

&lt;p&gt;For long videos (like movies), they contain much more information than what several sentences can describe, which poses challenges to video captioning. And they have a relatively large and non-trivial search space for visual question answering. Given the large number of short videos, a moderate next step is to work on these tasks using short videos.&lt;/p&gt;

&lt;p&gt;Modeling short videos can be done by combing CNNs and RNNs: each frame can be modeled by a CNN and the sequence of frames is well suited for an RNN. On the way to connect computer vision and natural language, short videos act as a good transition state between static images and long videos. And their popularity provides a lot of applications in recommendation, structured analysis, etc. Successful modeling of short videos will also be helpful to long videos since long videos can be treated as a sequence of relatively short videos (shots).&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Computer vision has been expected to be connected with natural language since born. And humans are good at both of these tasks. So an intelligent agent in the future should preferably have these two kinds of abilities. However, the two areas present two modalities of data, which poses a challenging modeling problem. In recent years, the success of CNNs and RNNs has solved the modeling problem much better than ever before. Based on these homogeneous network architectures, breakthroughs have already been achieved in tasks like image/video captioning, image generation and visual question answering, which all seek to build the connection in some way. These advancements open up more opportunities, like fine-grained image captioning for online advertising and modeling short videos. And efforts spent on solving these problems will become the next  ``small step&amp;rdquo; to enable the computer to describe what it sees.&lt;/p&gt;

&lt;h2 id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;If you find this topic (connecting computer vision and natural image) interesting to you, I strongly recommend you to read Andrej Karpathy&amp;rsquo;s PhD thesis - &lt;a href=&#34;https://cs.stanford.edu/people/karpathy/main.pdf&#34; target=&#34;_blank&#34;&gt;Connecting Images and Natural Language&lt;/a&gt;. Actually the title of this blog post is inspired by his thesis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Killing Pytorch Multi Gpu Training the Safe Way</title>
      <link>https://jianchao-li.github.io/post/killing-pytorch-multi-gpu-training-the-safe-way/</link>
      <pubDate>Fri, 02 Nov 2018 14:58:10 +0800</pubDate>
      
      <guid>https://jianchao-li.github.io/post/killing-pytorch-multi-gpu-training-the-safe-way/</guid>
      <description>

&lt;p&gt;As you may have noticed from the title, this post is somewhat different from my previous ones. I would like to talk about a PyTorch DataLoader issue I encountered recently. I feel like devoting a post to it because it has taken me long time to figure out how to fix it.&lt;/p&gt;

&lt;h2 id=&#34;memory-consumption&#34;&gt;Memory consumption&lt;/h2&gt;

&lt;p&gt;Since my last post on &lt;a href=&#34;https://jianchao-li.github.io/2018/09/15/understanding-fully-convolutional-networks/&#34; target=&#34;_blank&#34;&gt;FCNs&lt;/a&gt;, I have been working on semantic segmentation. Nowadays, we have deep neural networks for it, like the state-of-the-art &lt;a href=&#34;https://arxiv.org/abs/1612.01105&#34; target=&#34;_blank&#34;&gt;PSPNet&lt;/a&gt; from CVPR 2017.&lt;/p&gt;

&lt;p&gt;In practice, segmentation networks are &lt;strong&gt;much more memory-intensive&lt;/strong&gt; than recognition/classification networks. The reason is that semantic segmentation requires dense pixel-level predictions. For example, in the ImageNet classification task, you may use a neural network to transform a 224x224 image into 1000 real numbers (class probabilities). However, in semantic segmentation, suppose you have 20 semantic classes, you need to transform the 224x224 image into 20 224x224 probability maps, each representing probabilities of pixels belonging to one class. The output size changes from 1000 to 20x224x224=1003520, which is more than 1000 times!&lt;/p&gt;

&lt;p&gt;Besides the output, the intermediate feature maps in segmentation networks also consume more memory. In recognition networks, sizes of intermediate feature maps usually decrease monotically. However, since segmentation requires output of the same spatial dimension as the input, the feature maps will go through an extra process with their sizes increased (upsampled) back to the size of the input image. This extra upsample process further increases the memory consumption of segmentation networks.&lt;/p&gt;

&lt;p&gt;So, when we fit segmentation networks on a GPU, we need to reduce the batch size of the data. However, batch size is crucial to the performance of networks, especially those containing the batch normalization layer. Since no more data can be held in a single GPU, a natural soltuion is to use multiple GPUs and split the data across them (or more formally, &lt;em&gt;data parallelism&lt;/em&gt;).&lt;/p&gt;

&lt;h2 id=&#34;synchronized-batch-normalization&#34;&gt;Synchronized batch normalization&lt;/h2&gt;

&lt;p&gt;Here I would like to make a digression and mention an interesting layer, the synchronized batch normalization layer, which is introduced to increase the &lt;em&gt;working batch size&lt;/em&gt; for multi-GPU training. You may refer to the section &lt;strong&gt;Cross-GPU Batch Normalization&lt;/strong&gt; in &lt;a href=&#34;https://arxiv.org/abs/1711.07240&#34; target=&#34;_blank&#34;&gt;MegDet&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;When we use data parallelism to train on multiple GPUs, a batch of images will be splitted across several GPUs. Suppose your batch size is 16 (a common setting in semantic segmentation) and you train on 8 GPUs with data parallelism, then each GPU will have 2 images. A normal batch norm layer will only uses the 2 images on a single GPU to compute the mean and standard deviation, which is highly inaccurate and will make the training unstable.&lt;/p&gt;

&lt;p&gt;To effectively increase the working batch size, we need to synchronize all the GPUs in the batch norm layer, and fetch the mean and standard deviation computed at each GPU to compute a global value using all images. This is what synchronized batch norm layer does. If you would like to learn more about its implementation details, you may have a look at &lt;a href=&#34;https://github.com/vacancy/Synchronized-BatchNorm-PyTorch&#34; target=&#34;_blank&#34;&gt;Synchronized-BatchNorm-PyTorch&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-issue&#34;&gt;The Issue&lt;/h2&gt;

&lt;p&gt;After so much background information, the main idea is that semantic segmentation networks are very memory-intensive and require multiple GPUs to train a reasonable batch size. And synchronized batch norm can be used to increase the working batch size in multi-GPU training.&lt;/p&gt;

&lt;p&gt;Now comes the issue that I encountered recently. I was working with a semantic segmentation codebase written in PyTorch on a machine with 8 GPUs. The codebase incorporates synchronized batch norm and uses PyTorch multiprocessing for its custom DataLoader. I ran the training program for some time and then I killed it (I was running the program in a virtualized docker container in a cloud GPU cluster. So killing it is just to click a button in the cloud GUI).&lt;/p&gt;

&lt;p&gt;Then I checked the GPUs using &lt;code&gt;nvidia-smi&lt;/code&gt; and everything looked good.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.46                 Driver Version: 390.46                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|&lt;span style=&#34;color:#f92672&#34;&gt;===============================&lt;/span&gt;+&lt;span style=&#34;color:#f92672&#34;&gt;======================&lt;/span&gt;+&lt;span style=&#34;color:#f92672&#34;&gt;======================&lt;/span&gt;|
|   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;  Tesla V100-PCIE...  Off  | 00000000:1A:00.0 Off |                    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; |
| N/A   32C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;  Tesla V100-PCIE...  Off  | 00000000:1F:00.0 Off |                    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; |
| N/A   34C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;  Tesla V100-PCIE...  Off  | 00000000:20:00.0 Off |                    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; |
| N/A   33C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;  Tesla V100-PCIE...  Off  | 00000000:21:00.0 Off |                    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; |
| N/A   33C    P0    23W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;  Tesla V100-PCIE...  Off  | 00000000:B2:00.0 Off |                    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; |
| N/A   32C    P0    26W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;  Tesla V100-PCIE...  Off  | 00000000:B3:00.0 Off |                    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; |
| N/A   35C    P0    26W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;  Tesla V100-PCIE...  Off  | 00000000:B4:00.0 Off |                    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; |
| N/A   34C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; |
| N/A   35C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|&lt;span style=&#34;color:#f92672&#34;&gt;=============================================================================&lt;/span&gt;|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But then when I tried to start a new training program. An OOM error occurred. For the sake of privacy, some traceback logs were omitted by &lt;code&gt;...&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Traceback &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;most recent call last&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;:
  ...
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/site-packages/torch/cuda/streams.py&amp;#34;&lt;/span&gt;, line 21, in __new__
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; super&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;Stream, cls&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;.__new__&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cls, priority&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;priority, **kwargs&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
RuntimeError: CUDA error &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;2&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;: out of memory
Exception in thread Thread-1:
Traceback &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;most recent call last&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;:
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/threading.py&amp;#34;&lt;/span&gt;, line 916, in _bootstrap_inner
    self.run&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt;
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/threading.py&amp;#34;&lt;/span&gt;, line 864, in run
    self._target&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;*self._args, **self._kwargs&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
  ...
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/multiprocessing/queues.py&amp;#34;&lt;/span&gt;, line 337, in get
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; _ForkingPickler.loads&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;res&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py&amp;#34;&lt;/span&gt;, line 151, in rebuild_storage_fd
    fd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df.detach&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt;
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/multiprocessing/resource_sharer.py&amp;#34;&lt;/span&gt;, line 57, in detach
    with _resource_sharer.get_connection&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;self._id&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; as conn:
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/multiprocessing/resource_sharer.py&amp;#34;&lt;/span&gt;, line 87, in get_connection
    c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Client&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;address, authkey&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;process.current_process&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt;.authkey&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/multiprocessing/connection.py&amp;#34;&lt;/span&gt;, line 493, in Client
    answer_challenge&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;c, authkey&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/multiprocessing/connection.py&amp;#34;&lt;/span&gt;, line 732, in answer_challenge
    message &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; connection.recv_bytes&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;256&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;         &lt;span style=&#34;color:#75715e&#34;&gt;# reject large message&lt;/span&gt;
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/multiprocessing/connection.py&amp;#34;&lt;/span&gt;, line 216, in recv_bytes
    buf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self._recv_bytes&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;maxlength&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/multiprocessing/connection.py&amp;#34;&lt;/span&gt;, line 407, in _recv_bytes
    buf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self._recv&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;4&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
  File &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/local/lib/python3.6/multiprocessing/connection.py&amp;#34;&lt;/span&gt;, line 379, in _recv
    chunk &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; read&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;handle, remaining&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
ConnectionResetError: &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;Errno 104&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt; Connection reset by peer&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I ran &lt;code&gt;nvidia-smi&lt;/code&gt; again and everything still seemed good. So I wrote a &lt;code&gt;check.cu&lt;/code&gt; to check the GPU memory using CUDA APIs.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;#34;cuda.h&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;#34;cuda_runtime_api.h&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;namespace&lt;/span&gt; std;
  
&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;( &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; ) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; num_gpus;
    size_t free, total;
    cudaGetDeviceCount( &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;num_gpus );
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; ( &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; gpu_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; gpu_id &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; num_gpus; gpu_id&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt; ) {
        cudaSetDevice( gpu_id );
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; id;
        cudaGetDevice( &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;id );
        cudaMemGetInfo( &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;free, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;total );
        cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GPU &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; id &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; memory: free=&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; free &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, total=&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; total &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
    }
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, everything looked good.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ nvcc check.cu -o check &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; ./check
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; memory: free&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16488464384, total&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16945512448&lt;/span&gt;
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; memory: free&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16488464384, total&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16945512448&lt;/span&gt;
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; memory: free&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16488464384, total&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16945512448&lt;/span&gt;
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; memory: free&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16488464384, total&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16945512448&lt;/span&gt;
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; memory: free&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16488464384, total&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16945512448&lt;/span&gt;
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; memory: free&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16488464384, total&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16945512448&lt;/span&gt;
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; memory: free&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16488464384, total&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16945512448&lt;/span&gt;
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt; memory: free&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16488464384, total&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;16945512448&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since the error happened to PyTorch, I moved on to write a &lt;code&gt;check.py&lt;/code&gt;, which created a single-element PyTorch CUDA tensor for sanity check. And this script reproduced the OOM error.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
 
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;:
    num_gpus &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cuda&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;device_count()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; gpu_id &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(num_gpus):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
	    torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cuda&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_device(gpu_id)
	    torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, device&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;)
	    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;GPU {} is good&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(gpu_id))
        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;exec&lt;/span&gt;:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;GPU {} is bad: {}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(gpu_id, &lt;span style=&#34;color:#66d9ef&#34;&gt;exec&lt;/span&gt;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output was as follows: GPU 1 and 2 were OOM.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ python3 check.py 
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; is bad: CUDA error: out of memory
THCudaCheck FAIL file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/pytorch/aten/src/THC/THCGeneral.cpp line&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;663&lt;/span&gt; error&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; : out of memory
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; is bad: cuda runtime error &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;2&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; : out of memory at /pytorch/aten/src/THC/THCGeneral.cpp:663
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt; is good&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So, my GPUs 2 and 3 should be magically occupied by some zombie process. And I had to restart the machine to fix it. I think the zombie process was generated due to my incorrect way of killing the training program. So I decided not to use the kill button in the cloud GUI but logged into the docker container to kill it in the terminal.&lt;/p&gt;

&lt;p&gt;I searched on Google for how to kill a PyTorch multi-GPU training program. And I found &lt;a href=&#34;https://discuss.pytorch.org/u/smth/summary&#34; target=&#34;_blank&#34;&gt;@smth&lt;/a&gt;&amp;rsquo;s suggestion in &lt;a href=&#34;https://discuss.pytorch.org/t/pytorch-doesnt-free-gpus-memory-of-it-gets-aborted-due-to-out-of-memory-error/13775/14?u=jianchao-li&#34; target=&#34;_blank&#34;&gt;this reply&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;@rabst
so, I remember this issue. When investigating, we found that theres actually a bug in python multiprocessing that might keep the child process hanging around, as zombie processes.
It is not even visible to &lt;code&gt;nvidia-smi&lt;/code&gt; .
The solution is &lt;code&gt;killall python&lt;/code&gt; , or to &lt;code&gt;ps -elf | grep python&lt;/code&gt; and find them and &lt;code&gt;kill -9 [pid]&lt;/code&gt; to them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It explained why &lt;code&gt;nvidia-smi&lt;/code&gt; failed to reveal the memory issue. Great! But, the above commands did not work for me&amp;hellip;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Nothing is so fatiguing as the eternal haning on of an uncompleted task.
&lt;div style=&#34;text-align: right;&#34;&gt;&amp;mdash; William James&lt;/div&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;the-solution&#34;&gt;The Solution&lt;/h2&gt;

&lt;p&gt;After several days of searching, failing, searching again, failing again etc., I finally found one solution. It is just to find out the processes that occupied the GPUs and kill them. To find out those processes, I ran &lt;code&gt;fuser -v /dev/nvidia*&lt;/code&gt;, which listed all the processes that were occupying my NVIDIA GPUs. Since I have 8 GPUs, the output of this command is a bit log.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ fuser -v /dev/nvidia*
                     USER        PID ACCESS COMMAND
/dev/nvidia0:        root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F...m python3
/dev/nvidia1:        root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F...m python3
/dev/nvidia2:        root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F...m python3
/dev/nvidia3:        root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F...m python3
/dev/nvidia4:        root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F...m python3
/dev/nvidia5:        root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F...m python3
/dev/nvidia6:        root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F...m python3
/dev/nvidia7:        root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F...m python3
/dev/nvidiactl:      root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F...m python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F...m python3
/dev/nvidia-uvm:     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5416&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5417&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5418&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5419&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5420&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5421&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5422&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5423&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5424&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5425&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5426&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5427&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5428&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5429&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5430&lt;/span&gt; F.... python3
                     root       &lt;span style=&#34;color:#ae81ff&#34;&gt;5431&lt;/span&gt; F.... python3&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As can be seen from above, the main process had a PID of 5284. I spawned 16 workers for the DataLoader so there were 16 subprocesses whose PIDs were consecutive (from 5416 to 5431). First I used &lt;code&gt;kill -9&lt;/code&gt; to kill all of them. Then killed the main process.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; pid in &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;5416..5431&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; kill -9 $pid; &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# kill subprocesses&lt;/span&gt;
$ kill -9 &lt;span style=&#34;color:#ae81ff&#34;&gt;5284&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# kill main process&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After killing the subprocesses and main process, I ran &lt;code&gt;check.py&lt;/code&gt; again and this time every GPU was good.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ python3 check.py 
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; is good
GPU &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt; is good&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;another-trick&#34;&gt;Another Trick&lt;/h2&gt;

&lt;p&gt;If the above solution still does not work for you (it does happen to me sometimes), the following trick may be helpful. First, find out the training loop of your program. In most cases it will contain a loop based on the number of iterations. Then add the following code to that loop.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isfile(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;kill.me&amp;#39;&lt;/span&gt;):
    num_gpus &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cuda&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;device_count()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; gpu_id &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(num_gpus):
        torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cuda&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_device(gpu_id)
        torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cuda&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;empty_cache()
    exit(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Inside the &lt;code&gt;if&lt;/code&gt; statement, the code empties the caches of all GPUs and exits. After you add this code to the training iteration, once you want to stop it, just &lt;code&gt;cd&lt;/code&gt; into the directory of the training program and run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;touch kill.me&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then in the current or next iteration (based on whether the above code has been executed), the &lt;code&gt;if&lt;/code&gt; check will become true and all GPUs will be cleared and the program will exit. Since you directly tell Python to exit in the program, it will take care of everything for you. You may use anything instead of &lt;code&gt;kill.me&lt;/code&gt;. But just make sure it is special enough and thus you will not terminate the training inadvertently by creating a file with the same name.&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;The issue made me stuck for long time. And in this process of looking for the solution, I made some expensive trial and error. Several GPU servers in the cloud had a card OOM due to my incorrect way of killing the training program. And I had to ask the administrators to restart them. So I would definitely like others to avoid such a case.&lt;/p&gt;

&lt;p&gt;From another perspective, I would like to highlight the importance of engineering capabilities and experiences. Though I was working on semantic segmentation, I spent most of my time digging through all sorts of problems while running the multi-GPU codes.&lt;/p&gt;

&lt;p&gt;A final remark, I would not like to leave you an impression that I am blaming the issue on PyTorch, CUDA, the cloud GPU cluster, or any others. Actually it is mainly due to that I do not understand how PyTorch multi-GPU and multiprocessing work. And I think I will need to study these topics more systematically. Hopefully I will write a new post after learning more about them :-)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Fully Convolutional Networks</title>
      <link>https://jianchao-li.github.io/post/understanding-fully-convolutional-networks/</link>
      <pubDate>Sat, 15 Sep 2018 19:29:22 +0800</pubDate>
      
      <guid>https://jianchao-li.github.io/post/understanding-fully-convolutional-networks/</guid>
      <description>

&lt;p&gt;Fully convolutional networks, or FCNs, were proposed by &lt;a href=&#34;http://people.eecs.berkeley.edu/~jonlong/&#34; target=&#34;_blank&#34;&gt;Jonathan Long&lt;/a&gt;, &lt;a href=&#34;http://imaginarynumber.net/&#34; target=&#34;_blank&#34;&gt;Evan Shelhamer&lt;/a&gt; and &lt;a href=&#34;https://people.eecs.berkeley.edu/~trevor/&#34; target=&#34;_blank&#34;&gt;Trevor Darrell&lt;/a&gt; in CVPR 2015 as a framework for semantic segmentation.&lt;/p&gt;

&lt;h2 id=&#34;semantic-segmentation&#34;&gt;Semantic segmentation&lt;/h2&gt;

&lt;p&gt;Semantic segmentation is a task in which given an image, we need to assign a semantic label (like cat, dog, person, background etc.) to each of its pixels. The following are some examples taken from &lt;a href=&#34;http://host.robots.ox.ac.uk/pascal/VOC/&#34; target=&#34;_blank&#34;&gt;The PASCAL VOC data sets&lt;/a&gt; with different colors representing different semantic classes.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;pascalvoc.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;p&gt;The PASCAL VOC data sets define 20 semantic classes: aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, sofa, train, and tv/monitor. Actually these are all &lt;em&gt;object&lt;/em&gt; classes. For pixels falling into non-object classes (which are called &lt;em&gt;stuff&lt;/em&gt;) like sky, they will be labeled as &amp;ldquo;background&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Some later semantic segmentation data sets like &lt;a href=&#34;https://www.cityscapes-dataset.com/&#34; target=&#34;_blank&#34;&gt;The Cityscapes Dataset&lt;/a&gt; and &lt;a href=&#34;https://github.com/nightrome/cocostuff&#34; target=&#34;_blank&#34;&gt;The COCO-Stuff dataset&lt;/a&gt; account for both object and stuff classes. The following are some examples taken from the COCO-Stuff dataset, with class names also shown.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;cocostuff.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;h2 id=&#34;how-to-use-cnns-for-segmentation&#34;&gt;How to use CNNs for segmentation&lt;/h2&gt;

&lt;p&gt;Back in 2015, convolutional neural networks (CNNs) have achieved tremendous success in image classification (like &lt;a href=&#34;https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf&#34; target=&#34;_blank&#34;&gt;AlexNet&lt;/a&gt; and &lt;a href=&#34;https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf&#34; target=&#34;_blank&#34;&gt;GoogLeNet&lt;/a&gt;), doing extremely well in learning from a fixed-size image to a single category label. Generally, the image will go through several convolutional layers and then fully connected layers, giving a fixed-size (equal to the number of classes) output, whose softmax loss with respect to the ground truth label will be computed and back-propagated to update parameters.&lt;/p&gt;

&lt;p&gt;However, in semantic segmentation, we need pixel-wise dense predictions instead of just a single label. For example, for a 256x128 image, the segmentation mask is also of size 256x128. We need to use CNNs to generate a predicted mask and then compute the loss between it and the ground truth one.&lt;/p&gt;

&lt;p&gt;I was working on object segmentation at the end of 2014. Object segmentation is arguably much simpler than semantic segmentation in that it only classifies the pixels into two classes: the backround and the foreground (a specific object, like pedestrian, horse, bird etc.). The following are some examples taken from &lt;a href=&#34;http://www.vision.caltech.edu/visipedia/CUB-200-2011.html&#34; target=&#34;_blank&#34;&gt;Caltech-UCSD Birds-200-2011&lt;/a&gt; with the object masks shown below the images.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;cub200.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;p&gt;Since CNNs are good at handling fixed-size data, a simple idea is to fix both the sizes of the image and the object mask. Then we set the number of output units of the last fully connected layer to be equal to the number of mask elements.&lt;/p&gt;

&lt;p&gt;For loss computation, since I was working on object segmentation with only two values (0 for background and 1 for foreground) in the mask, I simply adopted the Euclidean loss. This extremely simple idea achieved state-of-the-art results in some object segmentation datasets and was publised in ICIP 2015 entitled &lt;a href=&#34;http://ieeexplore.ieee.org/document/7351084/&#34; target=&#34;_blank&#34;&gt;Object segmentation with deep regression&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;changing-fully-connected-layers-to-convolutional-layers&#34;&gt;Changing fully connected layers to convolutional layers&lt;/h2&gt;

&lt;p&gt;However, the above idea is just not elegant. It mechanically resizes the object mask to the designated size and flattens it into a vector as the regression target. For semantic segmentation, we will have one mask for each class. If we still would like to use the same idea, we need to flatten all the masks into vectors and concatenate them to be a giant vector for regression. For example, in PASCAL VOC, there are 21 classes. Suppose we set the mask size to be 64x64, then we need to regress a vector with 64x64x21=86016 elements using a fully connected layer. If the input to this layer is 512x14x14 (you will come across this size in the section of VGG), the weight matrix will have 512x14x14x86016=8631877632 (over 8.6 billion) elements! If represented using the 4-byte &lt;code&gt;float&lt;/code&gt;, this matrix alone will occupy more than 34 gigabytes! Well, let&amp;rsquo;s try to save some space of the parameters.&lt;/p&gt;

&lt;p&gt;A common idea to avoid the large number of parameters consumed by fully connected layers is to use convolutional layers. For example, we may use a convolutional layer to generate a 21x64x64 output feature map (we no longer need to flatten the masks into vectors). In this way, we only need 21 convolutional kernels, which will use considerably fewer number of parameters. Actually, each of the 64x64 positions in the 21 maps represent the probabilities of that location being each of the 21 classes. So we may compute a softmax loss for each of the position and take an average over all the positions as the final loss. Semantic segmentation actually can be treated as a pixel-wise classification task. Therefore, computing a softmax loss makes more sense than using the Euclidean loss.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s summarize the above idea. Given an image, we resize it to be, say, 224x224. And we resize the segmentation masks (suppose there are 21 of them) to be 64x64. Then we carefully design a convolutional network to transform the 3x224x224 image (3 represents the R, G, B channels) to a 21x64x64 feature map and compute softmax loss over it with which we can use back-propagation to learn the network parameters.&lt;/p&gt;

&lt;p&gt;I am not sure how you would perceive this idea. From my perspective, a natural question to task is, &lt;em&gt;why not just set the masks to be of the same size as the image&lt;/em&gt;. Well, we can actually make this happen by using same padding in all the convolutional layers and discarding all the pooling layers.&lt;/p&gt;

&lt;p&gt;However, CNNs typically include pooling layers to downsample the feature maps, which have shown the effectiveness in image classification and have become a must-do. So it is not a good idea to discard the pooling layers. Now, since pooling layers will reduce the size of feature maps, how can we enlarge them to be of the same size as the input image? In fully convolutional networks, the authors proposed to use &lt;em&gt;deconvolutional layers&lt;/em&gt; to &lt;strong&gt;upsample&lt;/strong&gt; the feature maps.&lt;/p&gt;

&lt;h2 id=&#34;deconvolutional-layers&#34;&gt;Deconvolutional layers&lt;/h2&gt;

&lt;p&gt;Actually deconvolution is a bad name as suggested by &lt;a href=&#34;https://arxiv.org/abs/1603.07285&#34; target=&#34;_blank&#34;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt; and it suggests to use &lt;em&gt;transpose convolution&lt;/em&gt;. There are also other names for your choice: &lt;em&gt;upconvolution&lt;/em&gt;, &lt;em&gt;fractionally strided convolution&lt;/em&gt;, and &lt;em&gt;backward strided convolution&lt;/em&gt;, as mentioned in &lt;a href=&#34;http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture11.pdf&#34; target=&#34;_blank&#34;&gt;CS231n 2018 Lecture 11&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In deep learning, deconvolution is just a convolution with the input size and output size swapped. In convolution, the relationship between the input size and the output size can be expressed as follows.&lt;/p&gt;

&lt;p&gt;$H_{out} = \frac{H_{in} + 2P - K}{S} + 1 \tag{1}\label{eq1}$&lt;/p&gt;

&lt;p&gt;$W_{out} = \frac{W_{in} + 2P - K}{S} + 1 \tag{2}\label{eq2}$&lt;/p&gt;

&lt;p&gt;$H_{out}$, $H_{in}$, $W_{out}$, $W_{in}$, $P$, $K$, and $S$ represent the output height, input height, output width, input width, padding, kernel size and stride respectively. In this post, we assume that the same padding and stride is used in both the height and width dimensions.&lt;/p&gt;

&lt;p&gt;For example, if you are convolving a 4x4 input ($H_{in} = W_{in} = 4$) with a 3x3 kernel ($K = 3$) with stride 1 ($S = 1$) and no padding ($P = 0$), you will get an output of size 2x2 ($\frac{4 + 2 \times 0 - 3}{1} + 1 = 2$). So this is a convolution that transforms a 4x4 input to a 2x2 output with a 3x3 convolutional kernel.&lt;/p&gt;

&lt;p&gt;Now, in deconvolution, we would like to transform a 2x2 input to a 4x4 output using the same 3x3 convolutional kernel. Since deconvolution is still convolution, equations $\eqref{eq1}$ and $\eqref{eq2}$ still hold. Suppose we also use stride 1, then we need to solve $4 = \frac{2 + 2P - 3}{1} + 1$, which gives $P = 2$. So the corresponding deconvolution is just a convolution of a 2x2 input with the same 3x3 kernel, stride 1, and padding 2.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;deconv.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;p&gt;Now let&amp;rsquo;s look at the relationship between the input size and output size in deconvolution. As mentioned above, deconvolution is a convolution with swapped input size and output size. So we can derive the relationship just by swapping $H_{in}$ with $H_{out}$ and $W_{in}$ with $W_{out}$ in equations $\eqref{eq1}$ and $\eqref{eq2}$, which gives&lt;/p&gt;

&lt;p&gt;$$H_{in} = \frac{H_{out} + 2P - K}{S} + 1 \tag{3}\label{eq3}$$&lt;/p&gt;

&lt;p&gt;$$W_{in} = \frac{W_{out} + 2P - K}{S} + 1 \tag{4}\label{eq4}$$&lt;/p&gt;

&lt;p&gt;By moving $H_{out}$ and $W_{out}$ to the left-hand side, we finally get&lt;/p&gt;

&lt;p&gt;$$H_{out} = SH_{in} + K - S - 2P \tag{5}\label{eq5}$$&lt;/p&gt;

&lt;p&gt;$$W_{out} = SW_{in} + K - S - 2P \tag{6}\label{eq6}$$&lt;/p&gt;

&lt;p&gt;Now you know how to design a deconvolutional layer to upsample an input size to a specific output size. With deconvolution, we can upsample downsampled feature maps back to the same size as the input image. I think deconvolution is indeed the key to FCNs. For more details of deconvolution, please refer to &lt;a href=&#34;https://arxiv.org/abs/1603.07285&#34; target=&#34;_blank&#34;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;vgg&#34;&gt;VGG&lt;/h2&gt;

&lt;p&gt;Before introducing FCNs, I would like to talk about &lt;a href=&#34;https://arxiv.org/pdf/1409.1556.pdf&#34; target=&#34;_blank&#34;&gt;VGG&lt;/a&gt;, which is the backbone network for the FCN example that will be presented in the next section. It was used by &lt;a href=&#34;http://www.robots.ox.ac.uk/~karen/&#34; target=&#34;_blank&#34;&gt;Karen Simonyan&lt;/a&gt; and &lt;a href=&#34;https://www.robots.ox.ac.uk/~az/&#34; target=&#34;_blank&#34;&gt;Andrew Zisserman&lt;/a&gt; in ILSVRC 2015 and won the second-place in the image classification task.&lt;/p&gt;

&lt;p&gt;Specifically, I will use the &lt;a href=&#34;https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-vgg_ilsvrc_16_layers_deploy-prototxt&#34; target=&#34;_blank&#34;&gt;16-layer VGG&lt;/a&gt; as an example. The following table is a breakdown of the network layer by layer. Note that the stride and padding is 1 and 0 by default if not specified. And the last softmax layer for loss computation is ignored. By &lt;strong&gt;Size&lt;/strong&gt;, we mean the shape of the output blobs, which is computed using equations $\eqref{eq1}$ and $\eqref{eq2}$.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Params&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3x224x224&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv1_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;64 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64x224x224&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu1_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64x224x224&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv1_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;64 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64x224x224&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu1_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64x224x224&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64x112x112&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv2_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;128 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;128x112x112&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu2_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;128x112x112&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv2_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;128 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;128x112x112&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu2_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;128x112x112&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;128x56x56&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv3_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;256 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256x56x56&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu3_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256x56x56&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv3_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;256 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256x56x56&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu3_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256x56x56&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv3_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;256 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256x56x56&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu3_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256x56x56&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;256x28x28&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv4_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x28x28&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu4_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x28x28&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv4_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x28x28&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu4_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x28x28&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv4_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x28x28&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu4_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x28x28&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x14x14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv5_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x14x14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu5_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x14x14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv5_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x14x14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu5_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x14x14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv5_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x14x14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu5_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x14x14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512x7x7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fc6&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;InnerProduct&lt;/td&gt;
&lt;td&gt;25088x4096 weight, 1x4096 bias&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4096&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu6&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4096&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;drop6&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Dropout&lt;/td&gt;
&lt;td&gt;p=0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4096&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fc7&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;InnerProduct&lt;/td&gt;
&lt;td&gt;4096x4096 weight, 1x4096 bias&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4096&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu7&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4096&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;drop7&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Dropout&lt;/td&gt;
&lt;td&gt;p=0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4096&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fc8&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;InnerProduct&lt;/td&gt;
&lt;td&gt;4096x1000 weight, 1x1000 bias&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As can be seen, VGG only uses 3x3 convolutional kernels and 2x2 pooling kernels with stride 2. This simple and homogeneous structure accounts for its popularity to some degree.&lt;/p&gt;

&lt;p&gt;There is a nice visualization of the 16-layer VGG in &lt;a href=&#34;http://ethereon.github.io/netscope/#/preset/vgg-16&#34; target=&#34;_blank&#34;&gt;netscope&lt;/a&gt;. By hovering your mouse over the layers, you will be able to see their parameters and output shapes, which should be the same to those in the above table. The &lt;a href=&#34;http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture09.pdf&#34; target=&#34;_blank&#34;&gt;CS231n 2018 Lecture 9&lt;/a&gt; also covers this popular network.&lt;/p&gt;

&lt;h2 id=&#34;fully-convolutional-networks&#34;&gt;Fully convolutional networks&lt;/h2&gt;

&lt;p&gt;Now you are ready to embrace the idea of FCNs. It is fairly simple: first downsample the image to smaller feature maps and then upsample them to the segmentation masks (of the same size as the image). &lt;a href=&#34;http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture11.pdf&#34; target=&#34;_blank&#34;&gt;CS231n 2018 Lecture 11&lt;/a&gt; has the following nice illustration which summarizes this process. Actually I think the $D_3 \times H/4 \times W/4$ of Low-res should be $D_3 \times H/8 \times W/8$. Anyway, you can just ignore the captions. The picture has reflected the core idea.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;fcn.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;p&gt;To gain more understanding, let&amp;rsquo;s walk through a concrete example - &lt;a href=&#34;https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/voc-fcn32s/val.prototxt&#34; target=&#34;_blank&#34;&gt;voc-fcn32s&lt;/a&gt;, an adaptation of the 16-layer VGG into an FCN for semantic segmentation in the PASCAL VOC data sets. Since this dataset has 21 classes, we need to learn 21 segmentation masks.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s also break the voc-fcn32s down layer by layer. Note that the size of &lt;code&gt;data&lt;/code&gt; is now $3 \times H \times W$. In this way, we will show that FCN is able to handle input of any size! All default settings are the same to those in the above table.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Params&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span style=&#34;color:red&#34;&gt;$3 \times H \times W$&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv1_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;64 3x3 kernels, &lt;span style=&#34;color:red&#34;&gt;padding 100&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$64 \times \left(H + 198\right) \times \left(W + 198\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu1_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$64 \times \left(H + 198\right) \times \left(W + 198\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv1_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;64 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$64 \times \left(H + 198\right) \times \left(W + 198\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu1_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$64 \times \left(H + 198\right) \times \left(W + 198\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$64 \times \left(\frac{H}{2} + 99\right) \times \left(\frac{W}{2} + 99\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv2_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;128 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$128 \times \left(\frac{H}{2} + 99\right) \times \left(\frac{W}{2} + 99\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu2_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$128 \times \left(\frac{H}{2} + 99\right) \times \left(\frac{W}{2} + 99\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv2_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;128 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$128 \times \left(\frac{H}{2} + 99\right) \times \left(\frac{W}{2} + 99\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu2_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$128 \times \left(\frac{H}{2} + 99\right) \times \left(\frac{W}{2} + 99\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$128 \times \left(\frac{H + 2}{4} + 49\right) \times \left(\frac{W + 2}{4} + 49\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv3_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;256 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$256 \times \left(\frac{H + 2}{4} + 49\right) \times \left(\frac{W + 2}{4} + 49\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu3_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$256 \times \left(\frac{H + 2}{4} + 49\right) \times \left(\frac{W + 2}{4} + 49\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv3_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;256 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$256 \times \left(\frac{H + 2}{4} + 49\right) \times \left(\frac{W + 2}{4} + 49\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu3_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$256 \times \left(\frac{H + 2}{4} + 49\right) \times \left(\frac{W + 2}{4} + 49\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv3_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;256 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$256 \times \left(\frac{H + 2}{4} + 49\right) \times \left(\frac{W + 2}{4} + 49\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu3_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$256 \times \left(\frac{H + 2}{4} + 49\right) \times \left(\frac{W + 2}{4} + 49\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$256 \times \left(\frac{H + 6}{8} + 24\right) \times \left(\frac{W + 6}{8} + 24\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv4_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{8} + 24\right) \times \left(\frac{W + 6}{8} + 24\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu4_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{8} + 24\right) \times \left(\frac{W + 6}{8} + 24\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv4_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{8} + 24\right) \times \left(\frac{W + 6}{8} + 24\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu4_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{8} + 24\right) \times \left(\frac{W + 6}{8} + 24\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv4_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{8} + 24\right) \times \left(\frac{W + 6}{8} + 24\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu4_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{8} + 24\right) \times \left(\frac{W + 6}{8} + 24\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv5_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu5_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv5_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu5_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv5_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;512 3x3 kernels, padding 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu5_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{32} + 6\right) \times \left(\frac{W + 6}{32} + 6\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fc6&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&#34;color:red&#34;&gt;Convolution&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;4096 7x7 kernels&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$4096 \times \frac{H + 6}{32} \times \frac{W + 6}{32}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu6&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$4096 \times \frac{H + 6}{32} \times \frac{W + 6}{32}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;drop6&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Dropout&lt;/td&gt;
&lt;td&gt;p=0.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$4096 \times \frac{H + 6}{32} \times \frac{W + 6}{32}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fc7&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&#34;color:red&#34;&gt;Convolution&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;4096 1x1 kernels&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$4096 \times \frac{H + 6}{32} \times \frac{W + 6}{32}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;relu7&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$4096 \times \frac{H + 6}{32} \times \frac{W + 6}{32}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;drop7&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Dropout&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$4096 \times \frac{H + 6}{32} \times \frac{W + 6}{32}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_fr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;21 1x1 kernels&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \frac{H + 6}{32} \times \frac{W + 6}{32}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;upscore&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&#34;color:red&#34;&gt;Deconvolution&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;21 64x64 kernels, stride 32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span style=&#34;color:red&#34;&gt;$21 \times \left(H + 38\right) \times \left(W + 38\right)$&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;span style=&#34;color:red&#34;&gt;Crop&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Explained below&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times H \times W$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Several interesting facts worth notice have been highlighted in red. Let&amp;rsquo;s go over them one by one.&lt;/p&gt;

&lt;p&gt;The most interesting and confusing one is probably the padding 100 in &lt;code&gt;conv1_1&lt;/code&gt;. Why do FCNs use padding 100 instead of just 1 as does VGG? Well, let&amp;rsquo;s try to use padding 1 and see what will happen. Using equations $\eqref{eq1}$ and $\eqref{eq2}$ repeatedly, we can compute that the corresponding output size of &lt;code&gt;pool5&lt;/code&gt; will be $512 \times \frac{H}{32} \times \frac{W}{32}$.&lt;/p&gt;

&lt;p&gt;So far so good. But now comes &lt;code&gt;fc6&lt;/code&gt; with 4096 7x7 kernels. By plugging the variables into $\eqref{eq1}$ and $\eqref{eq2}$, the output size of &lt;code&gt;fc6&lt;/code&gt; will be $4096 \times \frac{H - 192}{32} \times \frac{W - 192}{32}$. To make $\frac{H - 192}{32}$ and $\frac{W - 192}{32}$ positive (at least 1), both $H$ and $W$ should be greater than or equal to 224. This means that if we use padding 1 in &lt;code&gt;conv1_1&lt;/code&gt;, the FCN will only be able to handle images not smaller than 224x224. However, we would like FCN to be able to handle input of any size, which is one of its main advantages. So we need to add more padding in &lt;code&gt;conv1_1&lt;/code&gt; and 100 is a sensible value.&lt;/p&gt;

&lt;p&gt;We also see that both &lt;code&gt;fc6&lt;/code&gt; and &lt;code&gt;fc7&lt;/code&gt; are now convolutional layers, fitting the name &lt;em&gt;fully convolutional networks&lt;/em&gt;. In the deconvolutional layer &lt;code&gt;upscore&lt;/code&gt;, the feature maps of &lt;code&gt;score_fr&lt;/code&gt;with size $\frac{H + 6}{32}$x$\frac{W + 6}{32}$ are upsampled to $\left(H + 38\right) \times \left(W + 38\right)$. You may try to verify the correctness of this output size using equations $\eqref{eq5}$ and $\eqref{eq6}$.&lt;/p&gt;

&lt;p&gt;After &lt;code&gt;upscore&lt;/code&gt;, we have an output feature map of $21 \times \left(H + 38\right) \times \left(W + 38\right)$. However, what we want is $21 \times H \times W$. So here comes the last but not least Crop layer, which is used to &lt;em&gt;crop&lt;/em&gt; the input and defined as follows in Caffe.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-protobuf&#34; data-lang=&#34;protobuf&#34;&gt;layer {&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;  name&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;score&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;  type&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Crop&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;  bottom&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;upscore&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;  bottom&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;  top&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;score&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;  crop_param {&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;    axis&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;    offset&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;19&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;  }&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This Crop layer accepts &lt;code&gt;upscore&lt;/code&gt; ($21 \times \left(H + 38\right) \times \left(W + 38\right)$) and &lt;code&gt;data&lt;/code&gt; ($3 \times H \times W$) from the two &lt;code&gt;bottom&lt;/code&gt; fields. It also has two parameters: &lt;code&gt;axis: 2&lt;/code&gt; and &lt;code&gt;offset: 19&lt;/code&gt;. In Caffe, a feature map (blob) is of size $N \times C \times H \times W$, with $N$, $C$, $H$ and $W$ being the 0th, 1st, 2nd and 3rd dimension. So &lt;code&gt;upscore&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt; are actually of size $N \times 21 \times \left(H + 38\right) \times \left(W + 38\right)$ and $N \times 3 \times H \times W$ respectively. &lt;code&gt;axis: 2&lt;/code&gt; means to crop from the 2nd dimension (inclusive). So only the dimension $H$ and $W$ of &lt;code&gt;upscore&lt;/code&gt; ($\left(H + 38\right) \times \left(W + 38\right)$) will be cropped to be the same as &lt;code&gt;data&lt;/code&gt; ($H \times W$). And &lt;code&gt;offset: 19&lt;/code&gt; specifies the starting index of the cropping, which means that &lt;code&gt;upscore&lt;/code&gt; will be cropped to be &lt;code&gt;upscore[19: 19 + H, 19: 19 + W]&lt;/code&gt;, literally the central part of &lt;code&gt;upscore&lt;/code&gt;. The following is an illustration of this process, with the green part being the cropped region &lt;code&gt;score&lt;/code&gt;.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;crop.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;h2 id=&#34;simplifying-fcn-to-a-stack-of-convolutional-layers&#34;&gt;Simplifying FCN to a stack of convolutional layers&lt;/h2&gt;

&lt;p&gt;As shown in the above example, we use a Crop layer with &lt;code&gt;offset: 19&lt;/code&gt; to crop the feature maps. This cropping layer comes into use since sometimes the deconvolutional (upsampling) layer may not precisely generate an $H \times W$ feature map. Instead, it may give us something like $\left(H + 2T\right) \times \left(W + 2T\right)$. In this case we need to determine the offset $T$ to do the cropping.&lt;/p&gt;

&lt;p&gt;In the previous section, we break down the network layer by layer and write down the output shape for each layer, based on which we compute the offset of the Crop layer. In the coming sections, we will look at a general case and derive the offset.&lt;/p&gt;

&lt;p&gt;In this section, we first simplify an FCN into a stack of $n$ convolutional layers, as shown below. This will make later derivation easier.&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\overset{\text{Input}}{\longrightarrow}\fbox{conv-1}\longrightarrow\fbox{conv-2}\longrightarrow\dots\longrightarrow\fbox{conv-n}\overset{\text{Output}}{\longrightarrow}
\end{align}
$$&lt;/p&gt;

&lt;p&gt;However, FCNs also have other layers like pooling layers, deconvolutional layers, or ReLU layers. So why do we only consider convolutional layers?&lt;/p&gt;

&lt;p&gt;Well, if you have walked through the computation of the offset in voc-fcn32s, you will notice that the offset is only related to the size ($H$ and $W$) of the feature maps. And in FCNs, only convolutional layers, deconvolutional layers and pooling layers will change the feature map size. So we can safely ignore other layers like ReLU and Dropout.&lt;/p&gt;

&lt;p&gt;For deconvolutional layers, they are just convolutional layers. So we only need to check pooling layers. For pooling layers, they are actually equivalent to convolutional layers regarding the size relationship between the input and output. Specifically, for pooling layers, equations $\eqref{eq1}$ and $\eqref{eq2}$ exactly hold true. For example, in &lt;code&gt;pool1&lt;/code&gt;, we use 2x2 max pooling kernels ($K = 2$) with stride 2 ($S = 2$). And the default padding is 0 ($P = 0$). According to $\eqref{eq1}$ and $\eqref{eq2}$, we have&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
H_{out} &amp;amp;= \frac{H_{in} + 2 \times 0 - 2}{2} + 1 \\&lt;br /&gt;
&amp;amp;= \frac{H_{in}}{2}
\end{aligned}
\end{equation}\tag{7}\label{eq7}
$$&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
W_{out} &amp;amp;= \frac{W_{in} + 2 \times 0 - 2}{2} + 1 \\&lt;br /&gt;
&amp;amp;= \frac{W_{in}}{2}
\end{aligned}
\end{equation}\tag{8}\label{eq8}
$$&lt;/p&gt;

&lt;p&gt;which match our expectation to downsample the input by a factor of 2.&lt;/p&gt;

&lt;p&gt;So it makes sense to simplify an FCN to be a stack of convolutional layers since we only care about the size of the feature maps.&lt;/p&gt;

&lt;h2 id=&#34;reparameterizing-convolutional-layers&#34;&gt;Reparameterizing convolutional layers&lt;/h2&gt;

&lt;p&gt;Now, we only need to deal with convolutional layers. But, before diving into the derivation, let&amp;rsquo;s further simplify it by reparameterizing convolution. Specifically, we rewrite equations $\eqref{eq1}$ and $\eqref{eq2}$ by moving $H_{in}$ and $W_{in}$ to the left-hand side.&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
H_{in} &amp;amp;= S\left(H_{out} - 1\right) + K - 2P \\&lt;br /&gt;
&amp;amp;= SH_{out} + 2\left(\frac{K - S}{2} - P\right) \\&lt;br /&gt;
&amp;amp;= SH_{out} + 2P^\prime
\end{aligned}
\end{equation}\tag{9}\label{eq9}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
W_{in} &amp;amp;= S\left(W_{out} - 1\right) + K - 2P \\&lt;br /&gt;
&amp;amp;= SW_{out} + 2\left(\frac{K - S}{2} - P\right) \\&lt;br /&gt;
&amp;amp;= SW_{out} + 2P^\prime
\end{aligned}
\end{equation}\tag{10}\label{eq10}
$$&lt;/p&gt;

&lt;p&gt;As can be seen, we introduce a new parameter $P^\prime$ in equations $\eqref{eq9}$ and $\eqref{eq10}$, which is defined as follows.&lt;/p&gt;

&lt;p&gt;$$P^\prime = \frac{K - S}{2} - P \tag{11}\label{eq11}$$&lt;/p&gt;

&lt;p&gt;Given $P^\prime$, a convolutional layer with parameters $K$, $S$ and $P$ can be reparameterized by $S$ and $P^\prime$. $S$ still stands for the stride. And we name $P^\prime$ &lt;em&gt;offset&lt;/em&gt;. Notice that $P^\prime$ is the offset of a convoltional layer, which is different from the aforementioned $T$, the offset of the FCN.&lt;/p&gt;

&lt;p&gt;For pooling layers, equations $\eqref{eq9}$ and $\eqref{eq10}$ also apply to them exactly. For deconvolutional layers, we rewrite equations $\eqref{eq5}$ and $\eqref{eq6}$ by moving $H_{out}$ and $W_{out}$ to the left-hand side.&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
H_{out} &amp;amp;= SH_{in} + 2\left(\frac{K - S}{2} - P\right) \\&lt;br /&gt;
&amp;amp;= SH_{in} + 2P^\prime
\end{aligned}
\end{equation}\tag{12}\label{eq12}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
W_{out} &amp;amp;= SW_{in} + 2\left(\frac{K - S}{2} - P\right) \\&lt;br /&gt;
&amp;amp;= SW_{in} + 2P^\prime
\end{aligned}
\end{equation}\tag{13}\label{eq13}
$$&lt;/p&gt;

&lt;p&gt;Since a deconvolutional layer is just a convolutional layer with its input size and output size swapped. Let&amp;rsquo;s swap $H_{out}$ with $H_{in}$ and $W_{out}$ with $W_{in}$ in $\eqref{eq12}$ and $\eqref{eq13}$. Then we get the following convolutional layer expressed by equations $\eqref{eq14}$ and $\eqref{eq15}$.&lt;/p&gt;

&lt;p&gt;$$H_{in} = SH_{out} + 2P^\prime \tag{14}\label{eq14}$$&lt;/p&gt;

&lt;p&gt;$$W_{in} = SW_{out} + 2P^\prime \tag{15}\label{eq15}$$&lt;/p&gt;

&lt;p&gt;Similarly, we move $H_{out}$ and $W_{out}$ to the left-hand side.&lt;/p&gt;

&lt;p&gt;$$H_{out} = \frac{1}{S}H_{in} + 2\left(-\frac{P^\prime}{S}\right) \tag{16}\label{eq16}$$&lt;/p&gt;

&lt;p&gt;$$W_{out} = \frac{1}{S}W_{in} + 2\left(-\frac{P^\prime}{S}\right) \tag{17}\label{eq17}$$&lt;/p&gt;

&lt;p&gt;Note that equations $\eqref{eq12}$ and $\eqref{eq13}$ represent a deconvolutional layer with stride $S$ and offset $P^\prime$ while equations $\eqref{eq16}$ and $\eqref{eq17}$ represent a convolutional layer, whose stride and offset are $\frac{1}{S}$ and $-\frac{P^\prime}{S}$ respectively.&lt;/p&gt;

&lt;p&gt;Based on the above analysis, we can obtain the following theorem, which will come into use later.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; A deconvolution with stride $S$ and offset $P^\prime$ is equavilent to a convolution with stride $\frac{1}{S}$ and offset $-\frac{P^\prime}{S}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;computing-the-offset&#34;&gt;Computing the offset&lt;/h2&gt;

&lt;p&gt;Based on the above reparameterization of convolutional layers, the simplified FCN stack of convolutional layers can be represented as follows.&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\underset{H_0, W_0}{\overset{L_0}{\longrightarrow}}\underset{\text{conv-1}}{\boxed{S_1, P^\prime_1}}\underset{H_1, W_1}{\overset{L_1}{\longrightarrow}}\underset{\text{conv-2}}{\boxed{S_2, P^\prime_2}}\underset{H_2, W_2}{\overset{L_2}{\longrightarrow}}\dots\underset{H_{n - 1}, W_{n - 1}}{\overset{L_{n - 1}}{\longrightarrow}}\underset{\text{conv-n}}{\boxed{S_n, P^\prime_n}}\underset{H_n, W_n}{\overset{L_n}{\longrightarrow}}
\end{align}
$$&lt;/p&gt;

&lt;p&gt;The input to the network is denoted as $L_0$ and the output as $L_n$. For layer $L_i, i = 0, 1, 2, \dots, n$, its height, width, stride and offset are denoted as $H_i$, $W_i$, $S_i$ and $P^\prime_i$ respectively. Note that we ignore the $N$ and $C$ dimensions since the offset of FCN ($T$) is only related to $H$ and $W$. Let&amp;rsquo;s further assume that $H_0 = W_0$ such that $H_i = W_i$ for all $i = 0, 1, 2, \dots, n$. Now we only need to consider a single dimension $H$.&lt;/p&gt;

&lt;p&gt;Based on equations $\eqref{eq9}$ and $\eqref{eq10}$, we can write down&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
H_0 &amp;amp;= S_1H_1 + 2P^\prime_1 \\&lt;br /&gt;
H_1 &amp;amp;= S_2H_2 + 2P^\prime_2 \\&lt;br /&gt;
&amp;amp;\dots \\&lt;br /&gt;
H_{n - 1} &amp;amp;= S_nH_n + 2P^\prime_n
\end{aligned}
\end{equation}\tag{18}\label{eq18}
$$&lt;/p&gt;

&lt;p&gt;If we plug in the expression of $H_i$ into that of $H_{i - 1}$, we can get&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
H_0 &amp;amp;= S_1H_1 + 2P^\prime_1\\&lt;br /&gt;
&amp;amp;= S_1\left(S_2H_2 + 2P^\prime_2\right) + 2P^\prime_1 \\&lt;br /&gt;
&amp;amp;= \left(S_1S_2\right)H_2 + 2\left(S_1P^\prime_2 + P^\prime_1\right) \\&lt;br /&gt;
&amp;amp;= \left(S_1S_2\right)\left(S_3H_3 + 2P^\prime_3\right) + 2\left(S_1P^\prime_2 + P^\prime_1\right) \\&lt;br /&gt;
&amp;amp;= \left(S_1S_2S_3\right)H_3 + 2\left(S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right) \\&lt;br /&gt;
&amp;amp;= \dots
\end{aligned}
\end{equation}\tag{19}\label{eq19}
$$&lt;/p&gt;

&lt;p&gt;Have you noticed the regularities? If you move on, you will end up with&lt;/p&gt;

&lt;p&gt;$$
H_0 = \left(S_1S_2 \dots S_n\right)H_n + 2\left(S_1S_2 \dots S_{n - 1}P^\prime_n + S_1S_2 \dots S_{n - 2}P^\prime_{n - 1} + \dots + S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right) \tag{20}\label{eq20}$$&lt;/p&gt;

&lt;p&gt;According to the definition of $T$, we have&lt;/p&gt;

&lt;p&gt;$$H_n = H_0 + 2T \tag{21}\label{eq21}$$&lt;/p&gt;

&lt;p&gt;By plugging equation $\eqref{eq21}$ into equation $\eqref{eq20}$, we have&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
H_0 &amp;amp;= \left(S_1S_2 \dots S_n\right)\left(H_0 + 2T\right) + 2\left(S_1S_2 \dots S_{n - 1}P^\prime_n + S_1S_2 \dots S_{n - 2}P^\prime_{n - 1} + \dots + S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right) \\&lt;br /&gt;
&amp;amp;= \left(S_1S_2 \dots S_n\right)H_0 + 2\left(S_1S_2 \dots S_n\right)T + 2\left(S_1S_2 \dots S_{n - 1}P^\prime_n + S_1S_2 \dots S_{n - 2}P^\prime_{n - 1} + \dots + S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right)
\end{aligned}
\end{equation}\tag{22}\label{eq22}
$$&lt;/p&gt;

&lt;p&gt;Typically we design the network to make $S_1S_2 \dots S_n = 1$. Let&amp;rsquo;s take voc-fcn8s as an example to verify this point. In this network, we have the following general convolutional layers (both pooling and deconvolutional layers are also counted as convolutional layers).&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Old parameterization ($K, S, P$)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Reparameterization ($S, P^\prime$)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv1_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_1 = 3, S_1 = 1, P_1 = 100$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_1 = 1, P^\prime_1 = -99$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv1_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_2 = 3, S_2= 1, P_2 = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;\$S_2 = 1, P^\prime_2 = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_3 = 2, S_3 = 2, P_3 = 0$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_3 = 2, P^\prime_3 = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv2_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_4 = 3, S_4 = 1, P_4 = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_4 = 1, P^\prime_4 = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv2_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_5 = 3, S_5 = 1, P_5 = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_5 = 1, P^\prime_5 = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_6 = 2, S_6 = 2, P_6 = 0$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_6 = 2, P^\prime_6 = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv3_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_7 = 3, S_7 = 1, P_7 = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_7 = 1, P^\prime_7 = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv3_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_8 = 3, S_8 = 1, P_8 = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_8 = 1, P^\prime_8 = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv3_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_9 = 3, S_9 = 1, P_9 = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_9 = 1, P^\prime_9 = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{10} = 2, S_{10} = 2, P_{10} = 0$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{10} = 2, P^\prime_{10} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv4_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{11} = 3, S_{11} = 1, P_{11} = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{11} = 1, P^\prime_{11} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv4_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{12} = 3, S_{12} = 1, P_{12} = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{12} = 1, P^\prime_{12} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv4_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{13} = 3, S_{13} = 1, P_{13} = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{13} = 1, P^\prime_{13} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{14} = 2, S_{14} = 2, P_{14} = 0$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{14} = 2, , P^\prime_{14} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv5_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{15} = 3, S_{15} = 1, P_{15} = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{15} = 1, P^\prime_{15} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv5_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{16} = 3, S_{16} = 1, P_{16} = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{16} = 1, P^\prime_{16} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;conv5_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{17} = 3, S_{17} = 1, P_{17} = 1$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{17} = 1, P^\prime_{17} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{18} = 2, S_{18} = 2, P_{18} = 0$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{18} = 2, P^\prime_{18} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fc6&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{19} = 7, S_{19} = 1, P_{19} = 0$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{19} = 1, P^\prime_{19} = 3$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fc7&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{20} = 1, S_{20} = 1, P_{20} = 0$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{20} = 1, P^\prime_{20} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_fr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{21} = 1, S_{21} = 1, P_{21} = 0$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{21} = 1, P^\prime_{21} = 0$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;upscore&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deconvolution&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$K_{22} = 64, S_{22} = 32, P_{22} = 0$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$S_{22} = \frac{1}{32}, P^\prime_{22} = -\frac{1}{2}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For &lt;code&gt;upscore&lt;/code&gt;, it is a deconvolutional layer and so we make use of &lt;strong&gt;Theorem&lt;/strong&gt; to compute its reparameterization parameters. Multiplying the above $S_1S_2 \dots S_{22}$ will give us $2 ^ 5 \times 1 ^ {16} \times \frac{1}{32} = 1$.&lt;/p&gt;

&lt;p&gt;Given $S_1S_2 \dots S_n = 1$, equation $\eqref{eq22}$ will be simplified into&lt;/p&gt;

&lt;p&gt;$$H_0 = H_0 + 2T + 2\left(S_1S_2 \dots S_{n - 1}P^\prime_n + S_1S_2 \dots S_{n - 2}P^\prime_{n - 1} + \dots + S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right) \tag{23}\label{eq23}$$&lt;/p&gt;

&lt;p&gt;Now we can derive the equation for computing the offset $T$.&lt;/p&gt;

&lt;p&gt;$$T=-\left(S_1S_2 \dots S_{n - 1}P^\prime_n + S_1S_2 \dots S_{n - 2}P^\prime_{n - 1} + \dots + S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right) \tag{24}\label{eq24}$$&lt;/p&gt;

&lt;p&gt;I computed $T$ for voc-fcn32s using the following Python codes according to equation $\eqref{eq24}$ and the result is 19.0, which is exactly the offset of the Crop layer.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; S &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; P &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;99&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;]
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; N &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;22&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; sum &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(N):
&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;     prod &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; p[i]
&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(i):
&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;         prod &lt;span style=&#34;color:#f92672&#34;&gt;*=&lt;/span&gt; s[j]
&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;     sum &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; prod
&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; T &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;sum
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(T)
&lt;span style=&#34;color:#ae81ff&#34;&gt;19.0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;how-mxnet-computes-the-offset&#34;&gt;How MXNet computes the offset&lt;/h2&gt;

&lt;p&gt;Now you know one way to compute $T$. I would like to tell you one more, which is used in &lt;a href=&#34;https://github.com/apache/incubator-mxnet/blob/master/example/fcn-xs/symbol_fcnxs.py&#34; target=&#34;_blank&#34;&gt;MXNet&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;From equation $\eqref{eq19}$, we can write down the equations of $H_0$ expressed in $H_i$ for all $i = 1, 2, \dots, n$.&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
H_0 &amp;amp;= S_1H_1 + 2P^\prime_1 \tag{25}\label{eq25} \\&lt;br /&gt;
H_0 &amp;amp;= \left(S_1S_2\right)H_2 + 2\left(S_1P^\prime_2 + P^\prime_1\right) \tag{26}\label{eq26} \\&lt;br /&gt;
H_0 &amp;amp;= \left(S_1S_2S_3\right)H_3 + 2\left(S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right) \tag{27}\label{eq27} \\&lt;br /&gt;
&amp;amp;\dots \\&lt;br /&gt;
H_0 &amp;amp;= \left(S_1S_2 \dots S_n\right)H_n + 2\left(S_1S_2 \dots S_{n - 1}P^\prime_n + S_1S_2 \dots S_{n - 2}P^\prime_{n - 1} + \dots + S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right) \tag{28}\label{eq28}
\end{align}
$$&lt;/p&gt;

&lt;p&gt;As aforementioned, eqution $\eqref{eq25}$ is a reparameterization of the convolutional layer conv-1 connecting $L_0$ and $L_1$. Obviously, equations $\eqref{eq26}$ to $\eqref{eq28}$ all have a similar form. We can actually treat them as a &lt;em&gt;compound convolutional layer&lt;/em&gt; connecting $L_0$ and $L_2, L_3, \dots, L_n$. Let&amp;rsquo;s call the compound convolutional layer connecting $L_0$ and $L_i \left(i = 1, 2, \dots, n\right)$ the $i$-th compound convolutional layer, whose compound stride $S_i^{\text{compound}}$ and compound offset $P_i^{\text{compound}}$ are as follows.&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
\left(S_1^{\text{compound}}, P_1^{\text{compound}}\right) &amp;amp;= \left(S_1, P^\prime_1\right) \\&lt;br /&gt;
\left(S_2^{\text{compound}}, P_2^{\text{compound}}\right) &amp;amp;= \left(S_1S_2, S_1P^\prime_2 + P^\prime_1\right) \\&lt;br /&gt;
\left(S_3^{\text{compound}}, P_3^{\text{compound}}\right) &amp;amp;= \left(S_1S_2S_3, S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right) \\&lt;br /&gt;
&amp;amp;\dots \\&lt;br /&gt;
\left(S_n^{\text{compound}}, P_n^{\text{compound}}\right) &amp;amp;= \left(S_1S_2 \dots S_n, S_1S_2 \dots S_{n - 1}P^\prime_n + S_1S_2 \dots S_{n - 2}P^\prime_{n - 1} + \dots + S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1\right)
\end{aligned}
\end{equation}\tag{29}\label{eq29}
$$&lt;/p&gt;

&lt;p&gt;As you may have noticed, $P_n^{\text{compound}}$ is just $-T$. If we can compute $P_n^{\text{compound}}$, then we know the value of $T$.&lt;/p&gt;

&lt;p&gt;Since $H_0 = 1 \cdot H_0 + 0$, let&amp;rsquo;s introduce two auxiliary variables $\left(S_0^{\text{compound}}, P_0^{\text{compound}}\right) = \left(1, 0\right)$.&lt;/p&gt;

&lt;p&gt;Now the problem is: given $\left(S_1, P^\prime_1\right), \left(S_2, P^\prime_2\right), \dots, \left(S_n, P^\prime_n\right)$ and $\left(S_0^{\text{compound}}, P_0^{\text{compound}}\right)$, how to compute $\left(S_i^{\text{compound}}, P_i^{\text{compound}}\right)$ for $i = 1, 2, \dots, n$.&lt;/p&gt;

&lt;p&gt;This problem can be further reduced to: given $\left(S_{i - 1}^{\text{compound}}, P_{i - 1}^{\text{compound}}\right)$ and $\left(S_i, P^\prime_i\right)$, how to compute $\left(S_{i}^{\text{compound}}, P_{i}^{\text{compound}}\right)$ for $i = 1, 2, \dots, n$.&lt;/p&gt;

&lt;p&gt;According to the expressions of $S_{i}^{\text{compound}}$ and $P_{i}^{\text{compound}}$, we have&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
S_{i}^{\text{compound}} &amp;amp;= S_1S_2 \dots S_i \\&lt;br /&gt;
&amp;amp;= \underbrace{S_1S_2 \dots S_{i - 1}}_{S_{i - 1}^{\text{compound}}}S_i \\&lt;br /&gt;
&amp;amp;= S_{i - 1}^{\text{compound}}S_i
\end{aligned}
\end{equation}\tag{30}\label{eq30}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\begin{aligned}
P_{i}^{\text{compound}} &amp;amp;= S_1S_2 \dots S_{i - 1}P^\prime_i + S_1S_2 \dots S_{i - 2}P^\prime_{i - 1} + \dots + S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1 \\&lt;br /&gt;
&amp;amp;= \underbrace{S_1S_2 \dots S_{i - 1}}_{S_{i - 1}^{\text{compound}}}P^\prime_i + \underbrace{S_1S_2 \dots S_{i - 2}P^\prime_{i - 1} + \dots + S_1S_2P^\prime_3 + S_1P^\prime_2 + P^\prime_1}_{P_{i - 1}^{\text{compound}}} \\&lt;br /&gt;
&amp;amp;= S_{i - 1}^{\text{compound}}P^\prime_i + P_{i - 1}^{\text{compound}}
\end{aligned}
\end{equation}\tag{31}\label{eq31}
$$&lt;/p&gt;

&lt;p&gt;Equations $\eqref{eq30}$ and $\eqref{eq31}$ are actually how &lt;a href=&#34;https://github.com/apache/incubator-mxnet/blob/master/example/fcn-xs/symbol_fcnxs.py&#34; target=&#34;_blank&#34;&gt;MXNet&lt;/a&gt; compute the compound stride and compound offset. Let&amp;rsquo;s dive into the codes to see how it is implemented.&lt;/p&gt;

&lt;p&gt;In function &lt;code&gt;filter_map&lt;/code&gt;, a convolution with parameters $K$ (&lt;code&gt;kernel&lt;/code&gt;), $S$ (&lt;code&gt;stride&lt;/code&gt;) and $P$ (&lt;code&gt;pad&lt;/code&gt;) is reparameterized to $S$ (&lt;code&gt;stride&lt;/code&gt;) and $P^\prime_i$. &lt;code&gt;(kernel-stride)/2-pad&lt;/code&gt; is just $P^\prime_i$ according to equation $\eqref{eq11}$.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;filter_map&lt;/span&gt;(kernel&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, stride&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, pad&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (stride, (kernel&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;stride)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;pad)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In function &lt;code&gt;inv_fp&lt;/code&gt;, a deconvolutional layer is transformed to an equivalent convolutional layer according to &lt;strong&gt;Theorem&lt;/strong&gt;. &lt;code&gt;fp_in&lt;/code&gt; just stores $\left(S, P^\prime\right)$.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;inv_fp&lt;/span&gt;(fp_in):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fp_in[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;fp_in[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fp_in[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In &lt;code&gt;compose_fp&lt;/code&gt;, equations $\eqref{eq30}$ and $\eqref{eq31}$ are implemented. &lt;code&gt;fp_first&lt;/code&gt; represents $\left(S_{i - 1}^{\text{compound}}, P_{i - 1}^{\text{compound}}\right)$ and &lt;code&gt;fp_second&lt;/code&gt; represents $\left(S_i, P^\prime_i\right)$. The returned result is $\left(S_i^{\text{compound}}, P_i^{\text{compound}}\right)$.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compose_fp&lt;/span&gt;(fp_first, fp_second):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (fp_first[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;fp_second[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], fp_first[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;fp_second[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;fp_first[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, in &lt;code&gt;compose_fp_list&lt;/code&gt;, $\left(S_{n}^{\text{compound}}, P_{n}^{\text{compound}}\right)$ are computed iteratively using $\left(S_1, P^\prime_1\right), \left(S_2, P^\prime_2\right), \dots, \left(S_n, P^\prime_n\right)$ (stored in &lt;code&gt;fp_list&lt;/code&gt;) and $\left(S_0^{\text{compound}}, P_0^{\text{compound}}\right)$ (&lt;code&gt;fp_out&lt;/code&gt;) by repeatedly calling &lt;code&gt;compose_fp&lt;/code&gt;. You may convince yourself of this point by manually running several steps of the &lt;code&gt;for&lt;/code&gt; loop.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
def compose_fp_list(fp_list):
    fp_out = (1.0, 0.0)
    for fp in fp_list:
        fp_out = compose_fp(fp_out, fp)
    return fp_out
&lt;/code&gt;`&lt;/p&gt;

&lt;h2 id=&#34;finer-details&#34;&gt;Finer Details&lt;/h2&gt;

&lt;p&gt;In the &lt;code&gt;upscore&lt;/code&gt; layer of voc-fcn32s, the feature maps are directly upsampled by a large factor of 32 (this is why it is named voc-fcn32s), which will produce relatively coarse predictions due to missing finer detils from intermediate resolutions. So, in &lt;a href=&#34;https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/voc-fcn16s/val.prototxt&#34; target=&#34;_blank&#34;&gt;voc-fcn16s&lt;/a&gt; and &lt;a href=&#34;https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/voc-fcn8s/val.prototxt&#34; target=&#34;_blank&#34;&gt;voc-fcn8s&lt;/a&gt;, the shrinked feature maps are upsampled more than once before being recovered to the size of the image.&lt;/p&gt;

&lt;p&gt;For &lt;a href=&#34;https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/voc-fcn16s/val.prototxt&#34; target=&#34;_blank&#34;&gt;voc-fcn16s&lt;/a&gt;, the feature maps from &lt;code&gt;score_fr&lt;/code&gt; will first be upsampled by a factor of 2 in &lt;code&gt;upscore2&lt;/code&gt;. Then, we generate another set of outputs from &lt;code&gt;pool4&lt;/code&gt; using convolution in &lt;code&gt;score_pool4&lt;/code&gt; and crop it to be the same size as that of &lt;code&gt;upscore2&lt;/code&gt; in &lt;code&gt;score_pool4c&lt;/code&gt;. Finally, we combine &lt;code&gt;upscore2&lt;/code&gt; and &lt;code&gt;score_pool4c&lt;/code&gt; using element-wise summation in &lt;code&gt;fuse_pool4&lt;/code&gt;, upsample it by a factor of 16 in &lt;code&gt;upscore16&lt;/code&gt; and crop it in &lt;code&gt;score&lt;/code&gt; to obtain the output. We show the network architecture for this process while omitting the previous layers in the following figure. Moreover, this process is broken down in the table below.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;fcn-voc16s.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Params&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;21 1x1 kernels&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_fr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;21 1x1 kernels&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \frac{H + 6}{32} \times \frac{W + 6}{32}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;upscore2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deconvolution&lt;/td&gt;
&lt;td&gt;21 4x4 kernels, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{16} + 2\right) \times \left(\frac{W + 6}{16} + 2\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_pool4c&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Crop&lt;/td&gt;
&lt;td&gt;axis 2, offset 5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{16} + 2\right) \times \left(\frac{W + 6}{16} + 2\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fuse_pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Eltwise&lt;/td&gt;
&lt;td&gt;sum&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{16} + 2\right) \times \left(\frac{W + 6}{16} + 2\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;upscore16&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deconvolution&lt;/td&gt;
&lt;td&gt;21 32x32 kernels, stride 16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(H + 54\right) \times \left(W + 54\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Crop&lt;/td&gt;
&lt;td&gt;axis 2, offset 27&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times H \times W$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As can be seen, the finer details from the intermediate resolution in &lt;code&gt;pool4&lt;/code&gt; are incorporated into later feature maps, which will produce finer outputs than those of fcn-voc32s. Actually, fcn-voc16s utilizes two resolutions of a factor of 16 and a factor of 32.&lt;/p&gt;

&lt;p&gt;We may combine more resolutions in the same way. In fcn-voc8s, we generate one more set of outputs from &lt;code&gt;pool3&lt;/code&gt; and combine it with later feature maps. The network architecture is similarly shown in the figure below with the process broken down in the following table.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;fcn-voc8s.png&#34; width=&#34;100%&#34;/&gt;&lt;/div&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Params&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$256 \times \left(\frac{H + 6}{8} + 24\right) \times \left(\frac{W + 6}{8} + 24\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_pool3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;21 1x1 kernels&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{8} + 24\right) \times \left(\frac{W + 6}{8} + 24\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pooling&lt;/td&gt;
&lt;td&gt;max 2x2, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$512 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;21 1x1 kernels&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{16} + 12\right) \times \left(\frac{W + 6}{16} + 12\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_fr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Convolution&lt;/td&gt;
&lt;td&gt;21 1x1 kernels&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \frac{H + 6}{32} \times \frac{W + 6}{32}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;upscore2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deconvolution&lt;/td&gt;
&lt;td&gt;21 4x4 kernels, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{16} + 2\right) \times \left(\frac{W + 6}{16} + 2\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_pool4c&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Crop&lt;/td&gt;
&lt;td&gt;axis 2, offset 5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{16} + 2\right) \times \left(\frac{W + 6}{16} + 2\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fuse_pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Eltwise&lt;/td&gt;
&lt;td&gt;sum&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{16} + 2\right) \times \left(\frac{W + 6}{16} + 2\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;upscore_pool4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deconvolution&lt;/td&gt;
&lt;td&gt;21 4x4 kernels, stride 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{8} + 6\right) \times \left(\frac{W + 6}{8} + 6\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score_pool3c&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Crop&lt;/td&gt;
&lt;td&gt;axis 2, offset 9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{8} + 6\right) \times \left(\frac{W + 6}{8} + 6\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fuse_pool3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Eltwise&lt;/td&gt;
&lt;td&gt;sum&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(\frac{H + 6}{8} + 6\right) \times \left(\frac{W + 6}{8} + 6\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;upscore8&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deconvolution&lt;/td&gt;
&lt;td&gt;21 16x16 kernels, stride 8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times \left(H + 62\right) \times \left(W + 62\right)$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;score&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Crop&lt;/td&gt;
&lt;td&gt;axis 2, offset 31&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$21 \times H \times W$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In fcn-voc8s, one more intermediate resolution &lt;code&gt;pool3&lt;/code&gt; are incorpoeated. From fcn-voc32s, fcn-voc16s to fcn-voc8s, more intermediate resolutions are incorporated and the results will contain more details, as shown below (taken from the &lt;a href=&#34;https://arxiv.org/pdf/1411.4038.pdf&#34; target=&#34;_blank&#34;&gt;FCN paper&lt;/a&gt;).&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;details.png&#34;/&gt;&lt;/div&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We cover fully convolutional networks in great detail. To summarize, we have learend:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Semantic segmentation requires dense pixel-level classification while image classification is only in image-level.&lt;/li&gt;
&lt;li&gt;Fully convolutional networks (FCNs) are a general framework to solve semantic segmentation.&lt;/li&gt;
&lt;li&gt;The key to generate outputs with the same size as the input in FCNs is to use deconvolution layers, which are just convolutional layers with input and output swapped.&lt;/li&gt;
&lt;li&gt;The offset parameter in the Crop layers of FCNs can be computed by breaking down the network layer by layer or using an analytic equation.&lt;/li&gt;
&lt;li&gt;Outputs with higher resolutions from intermediate layers of the network can be incorporated to enhance the details in the segmentation results.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Dive Into Visual Question Answering</title>
      <link>https://jianchao-li.github.io/post/a-dive-into-visual-question-answering/</link>
      <pubDate>Mon, 27 Aug 2018 14:34:35 +0800</pubDate>
      
      <guid>https://jianchao-li.github.io/post/a-dive-into-visual-question-answering/</guid>
      <description>

&lt;p&gt;I have been very interested in the interplay between vision and natural language for some time. In these years, an emerging research topic combined these two areas. That is, visual question answering (VQA). Recently, I made a dive into this topic and wrote some notes as you are reading now.&lt;/p&gt;

&lt;h2 id=&#34;what-is-vqa&#34;&gt;What is VQA?&lt;/h2&gt;

&lt;p&gt;VQA is a task that involves understanding the semantic information of both an image and a natuarl language question and returning the answer also expressed in natural language. You may play with the &lt;a href=&#34;http://demo.visualdialog.org/&#34; target=&#34;_blank&#34;&gt;Visual Chatbot&lt;/a&gt; to get a sense of VQA.&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;vqabot.png&#34; width=&#34;100%&#34; /&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;As can be seen, this is a multi-modal task involving two modes of data (an image and a text). To answer the question, both the semantics of the image and the question should be well understood.&lt;/p&gt;

&lt;h2 id=&#34;importance-of-vqa&#34;&gt;Importance of VQA&lt;/h2&gt;

&lt;p&gt;VQA is an important research topic, mainly for three reasons. The first is a historical one, kind of relevant to the origin of computer vision, a summer project at MIT back in 1966 [1]. Richard Szeliski wrote about this in his famous book [2]:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;in 1966, Marvin Minsky at MIT asked his undergraduate student Gerald Jay Sussman to spend the summer linking a camera to a computer and getting the computer to describe what it saw&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This &lt;em&gt;see-and-describe&lt;/em&gt; summarizes the original goal of the pioneers of computer vision: let the computer see the world around it (expressed in images) and describe it. In terms of this goal, a highly related task is image captioning, which I played with in &lt;a href=&#34;https://jianchao-li.github.io/2018/08/08/playing-with-image-captioning.html&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;. However, image captioning typically gives a general description of the image. If we would like the computer to describe some specific details, a natural way is to ask it to do so explicitly, which is what we do in VQA.&lt;/p&gt;

&lt;p&gt;The second reason that accounts for the significance of VQA is its potential to become an &lt;em&gt;AI-complete&lt;/em&gt; task [3]. Most tasks in artificial intelligence, especially computer vision, can be kind of boiled down to answering questions over images. For example, image classification is to answer a multiple-choice question of the category of an image.&lt;/p&gt;

&lt;p&gt;The last but ont least reason is that VQA has many promising applications. The most evident one is human-computer interaction, which benefits from VQA since it teaches a computer both to see and to speak. In the future, a human may be able to talk to an intelligent agent about a scene in natural language. This can further find many applications like navigation for the blind people (asking the navigation agent about what it sees to help the blind people know where to go) and video processing (asking an VQA agent to find out someone or something of interests in a large number of surveillance videos).&lt;/p&gt;

&lt;h2 id=&#34;breaking-down-vqa&#34;&gt;Breaking down VQA&lt;/h2&gt;

&lt;p&gt;Currently, researchers generally break the VQA problem down to four subproblems.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How to represent the image&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Convolutional neural newtorks (CNNs) have achieved great success in many image-related tasks. So many VQA pipelines make use of a pre-trained CNN to extract activations of specific layers as the image&amp;rsquo;s bottom-up features. A relatively new idea is to use some detection networks, like Faster R-CNN, to extract bottom-up attention features, as in the state-of-the-art [4].&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How to represent the question&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This subproblem is solved much better using LSTM, possibly with a concatenation with GloVe features.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How to combine the two representations&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are several possibilities of combing the two representations of images and questions: concatenation, element-wise multiplication/summation and outer product. Outer product is preferred since it allows all elements of the two representations to interact with each other. But it comes with a high dimension and thus large memory consumption and long computation time. A good solution to this problem is compact bilinear coding [5], which projects the outer product to a lower dimensional space.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How to generate the answer&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are mainly two ways: generating the answer using an RNN or by choosing it from a set of candidate answers as in classification. Most works use the classification approach, including the state-of-the-art [4].&lt;/p&gt;

&lt;h2 id=&#34;bottlenecks-of-vqa&#34;&gt;Bottlenecks of VQA&lt;/h2&gt;

&lt;p&gt;There are mainly two bottlenecks of the current VQA research.&lt;/p&gt;

&lt;p&gt;The first one is on the side of algorithms, specifically, the features of images/questions are computed in advance and then fed into the pipeline and fixed. This is kind of similar to the pre-deep-learning age of computer vision that researchers hand-engineered features (features were not learned end-to-end). It will be more preferable if the features can be learned by back-propagating answer errors to the input images and questions.&lt;/p&gt;

&lt;p&gt;The second one is on the side of datasets, specifically, the lack of datasets that ask questions which require external knowledge to answer. Incorporating external knowledge (like common sense or those from the encyclopedia) into VQA will push it to be an AI-complete task [3].&lt;/p&gt;

&lt;h2 id=&#34;thoughts-about-the-bottlnecks&#34;&gt;Thoughts about the bottlnecks&lt;/h2&gt;

&lt;p&gt;For the first bottleneck that features are not learned, one difficulty of learning those features for the image/question is that the pipeline includes some non-differentiable operations and thus back-propagation cannot be applied. An idea to overcome this difficulty is to use policy gradient [6].&lt;/p&gt;

&lt;p&gt;For the second bottleneck, the idea is to first collect a dataset for it. And the main challenge lies in how to incorporate the external knowledge into VQA. An idea, proposed in [7], is to learn a mapping from the image and question to a query into the knowledge database and incorporate the results of the query into the pipeline.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] S. Papert. The summer vision project. Technical Report Vision Memo. No. 100, Artificial Intelligence Group, Massachusetts Institute of Technology, 1966.&lt;/p&gt;

&lt;p&gt;[2] R. Szeliski. Computer Vision: Algorithms and Applications. Springer, 2010. &lt;a href=&#34;http://szeliski.org/Book/&#34; target=&#34;_blank&#34;&gt;http://szeliski.org/Book/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, and A. v. d. Hengel. Visual question answering: A survey of methods and datasets. Computer Vision and Image Understanding, 2017.&lt;/p&gt;

&lt;p&gt;[4] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.&lt;/p&gt;

&lt;p&gt;[5] A. Fukui, D. H. Park and D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach. Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding. CoRR abs/1606.01847, 2016.&lt;/p&gt;

&lt;p&gt;[6] J. Johnson, B. Hariharan, L. v. d. Maaten, J. Hoffman, L. Fei-Fei, C. L. Zitnick, and R. Girshick. Inferring and Executing Programs for Visual Reasoning. In Proceedings of the International Conference on Computer Vision, 2017.&lt;/p&gt;

&lt;p&gt;[7] P. Wang, Q. Wu, C. Shen, A. v. d. Hengel, A. Dick. FVQA: fact-based visual question answering. CoRR abs/1606.05433, 2016.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Playing With Image Captioning</title>
      <link>https://jianchao-li.github.io/post/playing-with-image-captioning/</link>
      <pubDate>Wed, 08 Aug 2018 20:12:45 +0800</pubDate>
      
      <guid>https://jianchao-li.github.io/post/playing-with-image-captioning/</guid>
      <description>

&lt;p&gt;I have been fascinated by image captioning for some time but still have not played with it. I gave it a try today using the open source project &lt;a href=&#34;https://github.com/karpathy/neuraltalk2&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;neuraltalk2&lt;/code&gt;&lt;/a&gt; written by &lt;a href=&#34;https://cs.stanford.edu/people/karpathy/&#34; target=&#34;_blank&#34;&gt;Andrej Karpathy&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-theory&#34;&gt;The theory&lt;/h2&gt;

&lt;p&gt;The working mechanism of image captioning is shown in the following picture (taken from &lt;a href=&#34;https://cs.stanford.edu/people/karpathy/&#34; target=&#34;_blank&#34;&gt;Andrej Karpathy&lt;/a&gt;).&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;rnn7.png&#34; width=&#34;50%&#34; /&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;The image is encoded into a feature vector by a convolutional neural network (CNN) and then fed into a recurrent neural network (RNN) to generate the captions. The RNN works word by word. Each time it receives an input word and a hidden state and generates the next word, which is used as the input word in the next time. The CNN feature vector of the image is used as the initial hidden state, which is updated in each time step of the RNN.&lt;/p&gt;

&lt;p&gt;In the above picture, the RNN receives the initial hidden state and &lt;code&gt;START&lt;/code&gt; (a special word incidating the RNN to start generation) and generates the first word &lt;code&gt;straw&lt;/code&gt;. Then &lt;code&gt;straw&lt;/code&gt; is fed into the RNN together with the updated hidden state to generate &lt;code&gt;hat&lt;/code&gt;. Finally, &lt;code&gt;hat&lt;/code&gt; is fed into the RNN with the latest hidden state to generate &lt;code&gt;END&lt;/code&gt; (a special word indicating the RNN to stop). So the caption of the image is &lt;em&gt;straw hat&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-experiment&#34;&gt;The experiment&lt;/h2&gt;

&lt;p&gt;I played with &lt;a href=&#34;https://github.com/karpathy/neuraltalk2&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;neuraltalk2&lt;/code&gt;&lt;/a&gt; to get a sense of how image captioning performs.&lt;/p&gt;

&lt;h3 id=&#34;working-environment&#34;&gt;Working environment&lt;/h3&gt;

&lt;p&gt;I ran the code in a VM instance of Google Cloud. If you also want to use Google Cloud, you may refer to the &lt;a href=&#34;http://cs231n.github.io/gce-tutorial/&#34; target=&#34;_blank&#34;&gt;Google Cloud Tutorial of CS231n&lt;/a&gt; to learn about how to set up a virtual instance. The tutorial is a bit long and you should only need to reach &lt;strong&gt;Connect to Your Virtual Instance&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The following screenshots show the settings of the VM instance. I made several changes:
* Changed Name to neuraltalk2
* Changed Region and Zone to us-west1 (Oregon) and us-west1-b
* Changed Boot disk to Ubuntu 16.04 LTS
* Checked Allow HTTP traffic and Allow HTTPS traffic&lt;/p&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;vm-up.png&#34; width=&#34;100%&#34; /&gt;&lt;/div&gt; &lt;br&gt;
&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;vm-down.png&#34; width=&#34;100%&#34; /&gt;&lt;/div&gt;

&lt;h3 id=&#34;installing-torch&#34;&gt;Installing Torch&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;neuraltalk2&lt;/code&gt; is written in Torch. So you need to install Torch first. You can simply follow the steps in &lt;a href=&#34;http://torch.ch/docs/getting-started.html#_&#34; target=&#34;_blank&#34;&gt;Getting started with Torch&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/torch/distro.git ~/torch --recursive
$ cd ~/torch; bash install-deps;
$ ./install.sh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the end of the last command, you will be prompted a question. Just answer &lt;code&gt;yes&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Do you want to automatically prepend the Torch install location
to PATH and LD_LIBRARY_PATH in your /home/jianchao/.bashrc? (yes/no)
[yes] &amp;gt;&amp;gt;&amp;gt;
yes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ source ~/.bashrc&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now Torch should be ready.&lt;/p&gt;

&lt;h3 id=&#34;installing-dependencies&#34;&gt;Installing dependencies&lt;/h3&gt;

&lt;p&gt;I ran &lt;code&gt;neuraltalk2&lt;/code&gt; on the CPU (since GPU is very expensive in Google Cloud). So I only need a part of the dependencies. I ran the following commands from my &lt;code&gt;$HOME&lt;/code&gt; directory to install the dependencies.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ luarocks install nn
$ luarocks install nngraph
$ luarocks install image

$ # Install Lua CJSON
$ wget https://www.kyne.com.au/~mark/software/download/lua-cjson-2.1.0.tar.gz
$ tar -xvzf lua-cjson-2.1.0.tar.gz
$ cd lua-cjson-2.1.0
$ luarocks make
$ cd # go back $HOME

$ # Install loadcaffe
$ sudo apt-get install libprotobuf-dev protobuf-compiler
$ CC=gcc-5 CXX=g++-5 luarocks install loadcaffe

$ # Install torch-hdf5
$ sudo apt-get install libhdf5-serial-dev hdf5-tools
$ git clone https://github.com/deepmind/torch-hdf5
$ cd torch-hdf5
$ luarocks make hdf5-0-0.rockspec LIBHDF5_LIBDIR=&amp;#34;/usr/lib/x86_64-linux-gnu/&amp;#34;
$ cd # go back $HOME&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that Andrej listed &lt;code&gt;loadcaffe&lt;/code&gt; and &lt;code&gt;torch-hdf5&lt;/code&gt; under &lt;strong&gt;For training&lt;/strong&gt;, but they are actually also required for inference. And if you woud like to run &lt;code&gt;neuraltalk2&lt;/code&gt; on a GPU, please follow the &lt;code&gt;README.md&lt;/code&gt; to install those additional dependencies.&lt;/p&gt;

&lt;h3 id=&#34;captioning-images&#34;&gt;Captioning images&lt;/h3&gt;

&lt;p&gt;Now we can use &lt;code&gt;neuraltalk2&lt;/code&gt; to caption images. Just clone the repository and download the pretrained model. Since I ran it on CPU, I downloaded the &lt;a href=&#34;https://cs.stanford.edu/people/karpathy/neuraltalk2/checkpoint_v1_cpu.zip&#34; target=&#34;_blank&#34;&gt;CPU model&lt;/a&gt;. You may need to download the &lt;a href=&#34;http://cs.stanford.edu/people/karpathy/neuraltalk2/checkpoint_v1.zip&#34; target=&#34;_blank&#34;&gt;GPU model&lt;/a&gt; to run it on GPU.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/karpathy/neuraltalk2.git
$ cd neuraltalk2
$ mkdir models
$ cd models
$ wget --no-check-certificate https://cs.stanford.edu/people/karpathy/neuraltalk2/checkpoint_v1_cpu.zip
$ unzip checkpoint_v1_cpu.zip&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I created another folder &lt;code&gt;images&lt;/code&gt; in the root directory of &lt;code&gt;neuraltalk2&lt;/code&gt; to store the test images. I downloaded two datasets for the experiment: the &lt;a href=&#34;2017 Val images [5K/1GB]&#34; target=&#34;_blank&#34;&gt;2017 Val Images of COCO&lt;/a&gt; and the &lt;a href=&#34;https://github.com/bearpaw/clothing-co-parsing&#34; target=&#34;_blank&#34;&gt;Clothing Co-Parsing (CCP) Dataset&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After everything is ready, just run the following command to apply &lt;code&gt;neuraltalk2&lt;/code&gt; to caption the images. Since I used CPU, I set &lt;code&gt;-gpuid -1&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;th eval.lua -model models/model_id1-501-1448236541.t7_cpu.t7 -image_folder images/ -num_images -1 -gpuid -1&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;

&lt;h4 id=&#34;coco&#34;&gt;COCO&lt;/h4&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;cococaps.png&#34; width=&#34;100%&#34; /&gt;&lt;/div&gt;

&lt;p&gt;In the COCO dataset, images are of various scenes and objects. And &lt;code&gt;neuraltalk2&lt;/code&gt; is able to capture the overall content of what is happening in the image, except for some mistakes like the cat is not sitting on the laptop. But, in general, the captions are very discriminative considering the large differences between images. Given images and captions, it is very easy to tell which image corresponds to which caption. Image captioning makes great sense in this case.&lt;/p&gt;

&lt;h4 id=&#34;ccp&#34;&gt;CCP&lt;/h4&gt;

&lt;div style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;ccpcaps.png&#34; width=&#34;100%&#34; /&gt;&lt;/div&gt;

&lt;p&gt;In the CCP dataset, images are all coming from the clothing domain and thus they are very similar to each other in the overall content. And the differences are mostly reflected in fine-grained details. In this case, the captions of &lt;code&gt;neuraltalk2&lt;/code&gt; which only capture the overall content become meaningless and are not very helpful for distinguishing one image from others. Moreover, the captions make more mistakes, like a lot of false positives of cell phones.&lt;/p&gt;

&lt;h3 id=&#34;thoughts&#34;&gt;Thoughts&lt;/h3&gt;

&lt;p&gt;For classifying images in the same domain, researchers have come up with fine-grained image classification. Now to caption these images, whose fine-grained details are much more important than the overall content, it makes sense to state that we need &lt;em&gt;fine-grained image captioning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To solve the fine-grained image captioning problem, we need to collect a dataset of images in the same domain with fine-grained captions. The considerable number of advertising captions for clothes/food/cars serve as a good basis. The pipeline of fine-grained image captioning may also be similar to that of general image captioning: a CNN learns a domain-specific representation of the image (maybe via fine-tuning the network in a fine-grained image classification task) and then an RNN generates a fine-grained caption conditioned on the representation. There should be many problems waiting to be discovered and solved in fine-grained image captioning.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
